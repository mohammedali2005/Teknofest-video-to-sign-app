{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12029170,"sourceType":"datasetVersion","datasetId":7568535}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# It's good practice to first uninstall potentially conflicting packages\n!pip uninstall torch torchvision torchaudio transformers accelerate bitsandbytes torchao -y\n\n# Install PyTorch (ensure compatibility with CUDA version on Kaggle GPU)\n!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n\n# Install OpenMIM\n!pip install -U openmim\n\n# Install MMEngine\n!mim install mmengine\n\n# Install a compatible version of MMCV as per the previous error message\n!pip uninstall mmcv -y # Uninstall current mmcv first to be sure\n!mim install \"mmcv>=2.0.0rc4,<2.2.0\" \n\n# Install specific, potentially older but more stable, versions of transformers and accelerate\n# These versions are chosen to reduce the likelihood of issues with very new torchao features.\n!pip install transformers==4.30.2 \n!pip install accelerate==0.22.0 \n\n# Now install mmdet and mmpose. These should pick up the already installed compatible libraries.\n!mim install \"mmdet>=3.0.0\" \n!mim install \"mmpose>=1.0.0\"\n\n# Install other necessary libraries\n!pip install opencv-python numpy tqdm pandas openpyxl requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T13:58:58.658394Z","iopub.execute_input":"2025-06-06T13:58:58.659110Z","iopub.status.idle":"2025-06-06T14:00:55.931090Z","shell.execute_reply.started":"2025-06-06T13:58:58.659076Z","shell.execute_reply":"2025-06-06T14:00:55.930270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport mmpose\nimport mmdet\nimport mmcv\nimport mmengine # Ensure this is imported\nimport cv2\nimport numpy as np\nimport os\nimport pandas as pd\nimport requests\nimport subprocess\nimport re\nfrom pathlib import Path\nfrom tqdm import tqdm\n# import tempfile # Not strictly needed now\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Torchvision version: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Current CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n\nprint(f\"MMPose version: {mmpose.__version__}\")\nprint(f\"MMDetection version: {mmdet.__version__}\")\nprint(f\"MMCV version: {mmcv.__version__}\")\nprint(f\"MMEngine version: {mmengine.__version__}\") # <<< ADD THIS LINE\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"Requests version: {requests.__version__}\")\nprint(f\"OpenCV version: {cv2.__version__}\")\n\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(f\"MMCV CUDA version: {get_compiling_cuda_version()}\")\nprint(f\"MMCV compiler version: {get_compiler_version()}\")\n\nffmpeg_check = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)\nif ffmpeg_check.returncode == 0:\n    print(\"ffmpeg found.\")\nelse:\n    print(\"ffmpeg not found. Segmentation will fail.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:25:49.473607Z","iopub.execute_input":"2025-06-06T14:25:49.473917Z","iopub.status.idle":"2025-06-06T14:25:49.579409Z","shell.execute_reply.started":"2025-06-06T14:25:49.473899Z","shell.execute_reply":"2025-06-06T14:25:49.578518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport requests\nimport torch # For torch.hub.download_url_to_file\nimport subprocess # For git clone\n\n# --- Directories ---\nBASE_WORKING_DIR = '/kaggle/working/'\nCHECKPOINTS_DIR = os.path.join(BASE_WORKING_DIR, 'checkpoints')\nMMDET_DIR = os.path.join(BASE_WORKING_DIR, 'mmdetection')\nMMPOSE_DIR = os.path.join(BASE_WORKING_DIR, 'mmpose')\n\nos.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n\n# --- Clone MMDetection and MMPose Repositories for Config Files ---\n# We'll clone specific versions/tags if known, otherwise main branch.\n# For mmdet>=3.0.0, let's try to get a recent stable tag or main.\n# For mmpose>=1.0.0, similar approach.\nMMDET_REPO_URL = \"https://github.com/open-mmlab/mmdetection.git\"\nMMPOSE_REPO_URL = \"https://github.com/open-mmlab/mmpose.git\"\nMMDET_TAG = \"v3.1.0\" # A recent stable tag for mmdet 3.x\nMMPOSE_TAG = \"v1.1.0\" # A recent stable tag for mmpose 1.x\n\n\ndef clone_repo_if_not_exists(repo_url, target_dir, tag=None):\n    if not os.path.exists(os.path.join(target_dir, '.git')): # Check if it's a git repo\n        print(f\"Cloning {repo_url} (tag: {tag if tag else 'latest'}) to {target_dir}...\")\n        clone_command = ['git', 'clone']\n        if tag:\n            clone_command.extend(['-b', tag])\n        clone_command.extend([repo_url, target_dir])\n        \n        try:\n            subprocess.run(clone_command, check=True, capture_output=True, text=True)\n            print(f\"Successfully cloned {repo_url} to {target_dir}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Error cloning {repo_url}:\")\n            print(f\"Command: {' '.join(e.cmd)}\")\n            print(f\"Return code: {e.returncode}\")\n            print(f\"Stdout: {e.stdout}\")\n            print(f\"Stderr: {e.stderr}\")\n            raise # Re-raise the exception to stop execution if cloning fails\n    else:\n        print(f\"Repository already exists at {target_dir}, skipping clone.\")\n\nclone_repo_if_not_exists(MMDET_REPO_URL, MMDET_DIR, MMDET_TAG)\nclone_repo_if_not_exists(MMPOSE_REPO_URL, MMPOSE_DIR, MMPOSE_TAG)\n\n\n# --- Pose Estimation Model (RTMPose-L Wholebody) ---\n# Config path now points within the cloned mmpose repo\nlocal_pose_config_file = os.path.join(MMPOSE_DIR, 'configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py')\npose_checkpoint_file_url = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-l_simcc-coco-wholebody_pt-aic-coco_270e-384x288-eaeb96c8_20230125.pth'\nlocal_pose_checkpoint_file = os.path.join(CHECKPOINTS_DIR, 'rtmpose-l_coco-wholebody.pth')\n\n# --- Object Detection Model (Faster R-CNN R50 FPN) ---\n# Config path now points within the cloned mmdetection repo\nlocal_det_config_file = os.path.join(MMDET_DIR, 'configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py')\ndet_checkpoint_file_url = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\nlocal_det_checkpoint_file = os.path.join(CHECKPOINTS_DIR, 'faster_rcnn_r50_fpn_1x_coco.pth')\n\n\ndef download_checkpoint_if_not_exists(url, local_path):\n    if not os.path.exists(local_path):\n        print(f\"Downloading checkpoint {url} to {local_path}...\")\n        try:\n            # Using torch.hub.download_url_to_file for checkpoints\n            torch.hub.download_url_to_file(url, local_path, progress=True)\n            print(\"Download complete.\")\n        except Exception as e:\n            print(f\"Error downloading checkpoint {url}: {e}\")\n            if os.path.exists(local_path): # Clean up partial download\n                os.remove(local_path)\n            raise\n    else:\n        print(f\"Checkpoint {local_path} already exists.\")\n\ndownload_checkpoint_if_not_exists(pose_checkpoint_file_url, local_pose_checkpoint_file)\ndownload_checkpoint_if_not_exists(det_checkpoint_file_url, local_det_checkpoint_file)\n\n# Verify that config files exist at their new paths\nif not os.path.exists(local_pose_config_file):\n    print(f\"ERROR: Pose config file not found: {local_pose_config_file}\")\n    print(\"Please check the mmpose repository structure and path.\")\nelse:\n    print(f\"Pose config file found: {local_pose_config_file}\")\n\nif not os.path.exists(local_det_config_file):\n    print(f\"ERROR: Detection config file not found: {local_det_config_file}\")\n    print(\"Please check the mmdetection repository structure and path.\")\nelse:\n    print(f\"Detection config file found: {local_det_config_file}\")\n    \nprint(\"\\nModel configuration paths updated to use cloned repos. Checkpoint paths set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:26:12.244936Z","iopub.execute_input":"2025-06-06T14:26:12.245711Z","iopub.status.idle":"2025-06-06T14:26:12.261996Z","shell.execute_reply.started":"2025-06-06T14:26:12.245658Z","shell.execute_reply":"2025-06-06T14:26:12.261241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmdet.apis import init_detector, inference_detector\nfrom mmpose.apis import init_model as init_pose_estimator\nfrom mmengine.registry import DefaultScope # Import DefaultScope class for its static methods\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\n# --- Manually Define COCO-WholeBody Dataset Info (133 keypoints) ---\n# (COCO_WHOLEBODY_KEYPOINT_NAMES and OFFICIAL_MMPOSE_COCO_WHOLEBODY_SKELETON_LINKS definitions remain here as before)\nCOCO_WHOLEBODY_KEYPOINT_NAMES = [\n    'kpt_0', 'kpt_1', 'kpt_2', 'kpt_3', 'kpt_4', 'kpt_5', 'kpt_6', 'kpt_7', 'kpt_8', 'kpt_9', \n    'kpt_10', 'kpt_11', 'kpt_12', 'kpt_13', 'kpt_14', 'kpt_15', 'kpt_16', 'kpt_17', 'kpt_18', 'kpt_19',\n    'kpt_20', 'kpt_21', 'kpt_22', 'kpt_23', 'kpt_24', 'kpt_25', 'kpt_26', 'kpt_27', 'kpt_28', 'kpt_29',\n    'kpt_30', 'kpt_31', 'kpt_32', 'kpt_33', 'kpt_34', 'kpt_35', 'kpt_36', 'kpt_37', 'kpt_38', 'kpt_39',\n    'kpt_40', 'kpt_41', 'kpt_42', 'kpt_43', 'kpt_44', 'kpt_45', 'kpt_46', 'kpt_47', 'kpt_48', 'kpt_49',\n    'kpt_50', 'kpt_51', 'kpt_52', 'kpt_53', 'kpt_54', 'kpt_55', 'kpt_56', 'kpt_57', 'kpt_58', 'kpt_59',\n    'kpt_60', 'kpt_61', 'kpt_62', 'kpt_63', 'kpt_64', 'kpt_65', 'kpt_66', 'kpt_67', 'kpt_68', 'kpt_69',\n    'kpt_70', 'kpt_71', 'kpt_72', 'kpt_73', 'kpt_74', 'kpt_75', 'kpt_76', 'kpt_77', 'kpt_78', 'kpt_79',\n    'kpt_80', 'kpt_81', 'kpt_82', 'kpt_83', 'kpt_84', 'kpt_85', 'kpt_86', 'kpt_87', 'kpt_88', 'kpt_89',\n    'kpt_90', 'kpt_91', 'kpt_92', 'kpt_93', 'kpt_94', 'kpt_95', 'kpt_96', 'kpt_97', 'kpt_98', 'kpt_99',\n    'kpt_100', 'kpt_101', 'kpt_102', 'kpt_103', 'kpt_104', 'kpt_105', 'kpt_106', 'kpt_107', 'kpt_108', 'kpt_109',\n    'kpt_110', 'kpt_111', 'kpt_112', 'kpt_113', 'kpt_114', 'kpt_115', 'kpt_116', 'kpt_117', 'kpt_118', 'kpt_119',\n    'kpt_120', 'kpt_121', 'kpt_122', 'kpt_123', 'kpt_124', 'kpt_125', 'kpt_126', 'kpt_127', 'kpt_128', 'kpt_129',\n    'kpt_130', 'kpt_131', 'kpt_132'\n]\nOFFICIAL_MMPOSE_COCO_WHOLEBODY_SKELETON_LINKS = [\n    [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12],\n    [5, 6], [5, 7], [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2],\n    [1, 3], [2, 4], [3, 5], [4, 6], [17, 18], [18, 19], [19, 20],\n    [20, 21], [21, 22], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28],\n    [28, 29], [23, 30], [30, 31], [31, 32], [32, 33], [23, 34], [34, 35],\n    [35, 36], [36, 37], [23, 38], [38, 39], [39, 40], [40, 41], [42, 43],\n    [43, 44], [44, 45], [45, 46], [42, 47], [47, 48], [48, 49], [49, 50],\n    [42, 51], [51, 52], [52, 53], [53, 54], [42, 55], [55, 56], [56, 57],\n    [57, 58], [59, 60], [60, 61], [61, 62], [62, 63], [59, 64], [64, 65],\n    [65, 66], [66, 67], [59, 68], [68, 69], [69, 70], [70, 71], [59, 72],\n    [72, 73], [73, 74], [74, 75], [59, 76], [76, 77], [77, 78], [78, 79],\n    [80, 81], [81, 82], [82, 83], [80, 84], [84, 85], [85, 86], [80, 87],\n    [87, 88], [88, 89], [80, 90], [91, 92], [92, 93], [93, 94], [94, 95],\n    [91, 96], [96, 97], [97, 98], [98, 99], [91, 100], [100, 101],\n    [101, 102], [102, 103], [91, 104], [104, 105], [105, 106],\n    [106, 107], [91, 108], [108, 109], [109, 110], [110, 111], [112, 113],\n    [113, 114], [114, 115], [115, 116], [112, 117], [117, 118],\n    [118, 119], [119, 120], [112, 121], [121, 122], [122, 123],\n    [123, 124], [112, 125], [125, 126], [126, 127], [127, 128],\n    [112, 129], [129, 130], [130, 131], [131, 132]\n]\n\n# --- Initialize models ---\n# Initialize detector within the 'mmdet' scope\nprint(\"Initializing detector...\")\nwith DefaultScope.overwrite_default_scope(scope_name='mmdet'): # CORRECTED: Using the classmethod context manager\n    detector = init_detector(local_det_config_file, local_det_checkpoint_file, device=device)\nprint(\"Detector initialized.\")\n\n# Initialize pose estimator within the 'mmpose' scope\nprint(\"Initializing pose estimator...\")\nwith DefaultScope.overwrite_default_scope(scope_name='mmpose'): # CORRECTED: Using the classmethod context manager\n    pose_estimator = init_pose_estimator(local_pose_config_file, local_pose_checkpoint_file, device=device)\n    \n    # --- Manually set the dataset_meta for COCO-WholeBody ---\n    print(\"Explicitly setting COCO-WholeBody dataset_meta...\")\n    \n    new_dataset_meta = {\n        'keypoint_names': COCO_WHOLEBODY_KEYPOINT_NAMES,\n        'num_keypoints': len(COCO_WHOLEBODY_KEYPOINT_NAMES),\n        'skeleton_links': OFFICIAL_MMPOSE_COCO_WHOLEBODY_SKELETON_LINKS,\n    }\n    \n    if hasattr(pose_estimator, 'dataset_meta') and pose_estimator.dataset_meta is not None:\n        pose_estimator.dataset_meta.update(new_dataset_meta)\n        print(\"Updated existing pose_estimator.dataset_meta.\")\n    else:\n        pose_estimator.dataset_meta = new_dataset_meta\n        print(\"Set new pose_estimator.dataset_meta.\")\n            \n    print(f\"Using {len(pose_estimator.dataset_meta['keypoint_names'])} keypoint names.\")\n    print(f\"Using {len(pose_estimator.dataset_meta['skeleton_links'])} skeleton links.\")\n\nprint(\"Pose estimator initialized.\")\nprint(\"Models loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T15:27:51.199345Z","iopub.execute_input":"2025-06-06T15:27:51.200110Z","iopub.status.idle":"2025-06-06T15:27:52.668351Z","shell.execute_reply.started":"2025-06-06T15:27:51.200089Z","shell.execute_reply":"2025-06-06T15:27:52.667430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmengine.registry import DefaultScope # Ensure DefaultScope is imported\nimport numpy as np # Ensure numpy is imported\nimport torch # Ensure torch is imported for type checking\n\ndef extract_pose_from_frame(frame_bgr, detector_model, pose_estimator_model, person_label_id=0, detection_threshold=0.5):\n    \"\"\"\n    Extracts whole-body pose from a single frame for one person.\n    Manages MMLab scopes using DefaultScope.overwrite_default_scope context manager.\n    Handles cases where pred_instances data might already be numpy arrays.\n    \"\"\"\n    \n    person_bboxes_np = None\n    combined_pose_data = None\n    \n    # --- Part 1: Object Detection (MMDetection) ---\n    with DefaultScope.overwrite_default_scope(scope_name='mmdet'):\n        det_results = inference_detector(detector_model, frame_bgr)\n        pred_instances = det_results.pred_instances\n        \n        person_indices = (pred_instances.labels == person_label_id) & (pred_instances.scores > detection_threshold)\n        person_bboxes_data = pred_instances.bboxes[person_indices] # Could be tensor or numpy\n\n        if len(person_bboxes_data) == 0:\n            return None \n\n        areas = (person_bboxes_data[:, 2] - person_bboxes_data[:, 0]) * (person_bboxes_data[:, 3] - person_bboxes_data[:, 1])\n        \n        # areas might be a tensor, ensure it's on CPU for argmax if so, then convert to numpy if needed for indexing\n        if isinstance(areas, torch.Tensor):\n            largest_person_idx = areas.cpu().numpy().argmax()\n        else: # Assuming numpy array\n            largest_person_idx = areas.argmax()\n            \n        main_person_bbox_data = person_bboxes_data[largest_person_idx:largest_person_idx+1]\n        \n        if len(main_person_bbox_data) == 0: # Should be main_person_bbox_data.shape[0] for numpy\n            return None\n\n        if isinstance(main_person_bbox_data, torch.Tensor):\n            person_bboxes_np = main_person_bbox_data.cpu().numpy()\n        else: # Assuming numpy array\n            person_bboxes_np = main_person_bbox_data\n\n\n    if person_bboxes_np is None or person_bboxes_np.shape[0] == 0:\n        return None\n\n    # --- Part 2: Pose Estimation (MMPose) ---\n    with DefaultScope.overwrite_default_scope(scope_name='mmpose'):\n        pose_results = inference_topdown(pose_estimator_model, frame_bgr, person_bboxes_np)\n        \n        if not pose_results:\n            return None \n            \n        data_sample = pose_results[0] # This is a PoseDataSample object\n        \n        # Access keypoints and scores from the PoseDataSample's pred_instances\n        # These might be Tensors or NumPy arrays depending on the exact version and internal processing\n        \n        keypoints_data = data_sample.pred_instances.keypoints[0] # Get the data for the first (and only) detected person\n        keypoint_scores_data = data_sample.pred_instances.keypoint_scores[0]\n\n        # Convert to NumPy array if they are PyTorch Tensors\n        if isinstance(keypoints_data, torch.Tensor):\n            keypoints_np = keypoints_data.cpu().numpy()\n        elif isinstance(keypoints_data, np.ndarray):\n            keypoints_np = keypoints_data\n        else:\n            print(f\"Warning: Unexpected type for keypoints_data: {type(keypoints_data)}\")\n            return None # Or handle error appropriately\n\n        if isinstance(keypoint_scores_data, torch.Tensor):\n            keypoint_scores_np = keypoint_scores_data.cpu().numpy()\n        elif isinstance(keypoint_scores_data, np.ndarray):\n            keypoint_scores_np = keypoint_scores_data\n        else:\n            print(f\"Warning: Unexpected type for keypoint_scores_data: {type(keypoint_scores_data)}\")\n            return None # Or handle error appropriately\n        \n        combined_pose_data = np.concatenate((keypoints_np, keypoint_scores_np[:, np.newaxis]), axis=1)\n    \n    return combined_pose_data\n\nprint(\"Pose extraction function defined (handles numpy/tensor for keypoints).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:34:04.544425Z","iopub.execute_input":"2025-06-06T14:34:04.545285Z","iopub.status.idle":"2025-06-06T14:34:04.556313Z","shell.execute_reply.started":"2025-06-06T14:34:04.545256Z","shell.execute_reply":"2025-06-06T14:34:04.555656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EXCEL_FILE_PATH = '/kaggle/input/aslvid/asllvd_signs_2024_06_27.xlsx'\nASLLVD_BASE_URL = \"http://csr.bu.edu/ftp/asl/asllvd/asl-data2/quicktime/\"\n\n# Directories for downloaded and processed files\nBASE_WORKING_DIR = Path('/kaggle/working/')\nFULL_VIDEO_DOWNLOAD_DIR = BASE_WORKING_DIR / 'full_source_videos'\nSEGMENTED_VIDEO_DIR = BASE_WORKING_DIR / 'segmented_sign_clips'\nPOSE_DATA_OUTPUT_DIR = BASE_WORKING_DIR / 'pose_data'\n\nFULL_VIDEO_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\nSEGMENTED_VIDEO_DIR.mkdir(parents=True, exist_ok=True)\nPOSE_DATA_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\ndef load_excel_data(excel_path, num_samples=None):\n    \"\"\"Loads data from the Excel file. Optionally selects a number of samples.\"\"\"\n    try:\n        df = pd.read_excel(excel_path)\n        print(f\"Successfully loaded Excel file. Found {len(df)} rows.\")\n        # Basic cleaning: remove rows where essential data might be missing\n        essential_cols = ['full video file', \n                          'start frame of the sign (relative to full videos)', \n                          'end frame of the sign (relative to full videos)',\n                          'Video ID number',\n                          'occurrence label']\n        df.dropna(subset=essential_cols, inplace=True)\n        print(f\"After dropping rows with NA in essential columns, {len(df)} rows remaining.\")\n\n        # Ensure frame numbers are integers\n        df['start frame of the sign (relative to full videos)'] = df['start frame of the sign (relative to full videos)'].astype(int)\n        df['end frame of the sign (relative to full videos)'] = df['end frame of the sign (relative to full videos)'].astype(int)\n\n        if num_samples is not None and num_samples < len(df):\n            print(f\"Selecting {num_samples} random samples.\")\n            return df.sample(n=num_samples, random_state=42) # Use a random_state for reproducibility\n        return df\n    except FileNotFoundError:\n        print(f\"Error: Excel file not found at {excel_path}\")\n        return pd.DataFrame()\n    except Exception as e:\n        print(f\"Error loading or processing Excel file: {e}\")\n        return pd.DataFrame()\n\ndef parse_full_video_filename(full_video_name_from_excel):\n    \"\"\"\n    Parses 'ASL_YYYY_MM_DD_scene<SCENE_NUM>-camera<CAM_NUM>.mov'\n    Returns (session_dir_name, scene_num_str, camera_num_str, actual_filename_on_server)\n    Example: ASL_2008_01_11_scene71-camera1.mov\n    -> (\"ASL_2008_01_11\", \"71\", \"1\", \"scene71-camera1.mov\")\n    \"\"\"\n    match = re.match(r'(ASL_\\d{4}_\\d{2}_\\d{2})_scene(\\d+)-camera(\\d+)\\.mov', full_video_name_from_excel)\n    if match:\n        session_dir = match.group(1)\n        scene_num = match.group(2)\n        camera_num = match.group(3)\n        actual_filename = f\"scene{scene_num}-camera{camera_num}.mov\"\n        return session_dir, scene_num, camera_num, actual_filename\n    else:\n        print(f\"Warning: Could not parse filename format: {full_video_name_from_excel}\")\n        return None, None, None, None\n\ndef download_full_video(video_info_tuple, download_dir):\n    \"\"\"Downloads the full video if it doesn't already exist.\"\"\"\n    session_dir_name, scene_num, camera_num, actual_filename = video_info_tuple\n    if not all(video_info_tuple): # Check if parsing failed\n        return None\n\n    # Construct the part of the URL specific to this video\n    # e.g. ASL_2008_01_11/scene71-camera1.mov\n    video_url_path = f\"{session_dir_name}/{actual_filename}\"\n    full_url = ASLLVD_BASE_URL + video_url_path\n    \n    local_video_path = Path(download_dir) / actual_filename\n    \n    if local_video_path.exists():\n        print(f\"Full video already exists: {local_video_path}\")\n        return local_video_path\n\n    print(f\"Downloading full video from {full_url} to {local_video_path}...\")\n    try:\n        response = requests.get(full_url, stream=True, timeout=300) # 5 min timeout\n        response.raise_for_status()\n        with open(local_video_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192 * 4): # 32KB chunks\n                f.write(chunk)\n        print(f\"Download complete: {local_video_path}\")\n        return local_video_path\n    except requests.exceptions.RequestException as e:\n        print(f\"Error downloading {full_url}: {e}\")\n        if local_video_path.exists():\n            local_video_path.unlink() # Remove partial download\n        return None\n\ndef segment_video_ffmpeg(full_video_path, output_segment_path, start_frame, end_frame):\n    \"\"\"Segments a video using ffmpeg. Frames are 1-based and inclusive.\"\"\"\n    # ffmpeg's trim filter: end_frame is exclusive, so add 1 if our end_frame is inclusive\n    # The filter expects frame numbers.\n    command = [\n        'ffmpeg',\n        '-i', str(full_video_path),\n        '-vf', f'trim=start_frame={start_frame-1}:end_frame={end_frame},setpts=PTS-STARTPTS', # trim frames are 0-indexed\n        '-an',  # No audio\n        '-y',   # Overwrite output files\n        str(output_segment_path)\n    ]\n    print(f\"Running ffmpeg command: {' '.join(command)}\")\n    try:\n        result = subprocess.run(command, capture_output=True, text=True, check=True)\n        print(f\"ffmpeg stdout: {result.stdout}\")\n        print(f\"ffmpeg stderr: {result.stderr}\") # ffmpeg often prints info to stderr\n        if output_segment_path.exists() and output_segment_path.stat().st_size > 0:\n            print(f\"Successfully segmented video to {output_segment_path}\")\n            return output_segment_path\n        else:\n            print(f\"Error: Segmented video not created or is empty: {output_segment_path}\")\n            return None\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during ffmpeg segmentation for {full_video_path}:\")\n        print(f\"Command: {' '.join(e.cmd)}\")\n        print(f\"Return code: {e.returncode}\")\n        print(f\"Stdout: {e.stdout}\")\n        print(f\"Stderr: {e.stderr}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred during ffmpeg segmentation: {e}\")\n        return None\n\nprint(\"Utility functions for data loading, download, and segmentation defined.\")\n# Test parser\ntest_fn = \"ASL_2008_01_11_scene71-camera1.mov\"\nparsed_info = parse_full_video_filename(test_fn)\nprint(f\"Parsed info for {test_fn}: {parsed_info}\")\nif parsed_info and all(parsed_info):\n    session, scene, cam, actual_fn = parsed_info\n    test_url = ASLLVD_BASE_URL + f\"{session}/{actual_fn}\"\n    print(f\"Constructed test URL: {test_url}\")\n\ntest_fn_2 = \"ASL_2008_03_13_scene111-camera2.mov\" # Example for camera2\nparsed_info_2 = parse_full_video_filename(test_fn_2)\nprint(f\"Parsed info for {test_fn_2}: {parsed_info_2}\")\nif parsed_info_2 and all(parsed_info_2):\n    session, scene, cam, actual_fn = parsed_info_2\n    test_url_2 = ASLLVD_BASE_URL + f\"{session}/{actual_fn}\"\n    print(f\"Constructed test URL for camera2: {test_url_2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:34:07.262098Z","iopub.execute_input":"2025-06-06T14:34:07.262417Z","iopub.status.idle":"2025-06-06T14:34:07.280802Z","shell.execute_reply.started":"2025-06-06T14:34:07.262397Z","shell.execute_reply":"2025-06-06T14:34:07.279852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_SIGNS_TO_PROCESS = 3  # Start with a small number of signs\nCLEANUP_TEMP_FILES = False # Set to False to keep downloaded/segmented videos for inspection\n\nexcel_data_df = load_excel_data(EXCEL_FILE_PATH, num_samples=NUM_SIGNS_TO_PROCESS)\n\nif excel_data_df.empty:\n    print(\"No data loaded from Excel. Exiting processing loop.\")\nelse:\n    print(f\"Processing {len(excel_data_df)} selected sign entries...\")\n    \n    # Ensure correct column names from your Excel file\n    # From your image: 'Video ID number', 'occurrence label', \n    # 'start frame of the sign (relative to full videos)', 'end frame of the sign (relative to full videos)',\n    # 'full video file'\n    col_video_id = 'Video ID number'\n    col_occurrence_label = 'occurrence label'\n    col_start_frame_sign = 'start frame of the sign (relative to full videos)'\n    col_end_frame_sign = 'end frame of the sign (relative to full videos)'\n    col_full_video_file = 'full video file'\n\n    for index, row in tqdm(excel_data_df.iterrows(), total=excel_data_df.shape[0], desc=\"Processing Signs\"):\n        video_id = str(row[col_video_id])\n        occurrence_label = str(row[col_occurrence_label]).replace('/', '_').replace('\\\\', '_').replace(' ', '') # Sanitize for filename\n        start_frame = int(row[col_start_frame_sign])\n        end_frame = int(row[col_end_frame_sign])\n        full_video_filename_excel = row[col_full_video_file]\n\n        print(f\"\\nProcessing sign: ID {video_id}, Label {occurrence_label}, Source {full_video_filename_excel}, Frames {start_frame}-{end_frame}\")\n\n        # 1. Parse filename and get download info\n        video_info_tuple = parse_full_video_filename(full_video_filename_excel)\n        if not all(video_info_tuple):\n            print(f\"Skipping due to filename parse error: {full_video_filename_excel}\")\n            continue\n        \n        # 2. Download full source video (if not already present)\n        downloaded_full_video_path = download_full_video(video_info_tuple, FULL_VIDEO_DOWNLOAD_DIR)\n        if not downloaded_full_video_path:\n            print(f\"Skipping due to download error for: {full_video_filename_excel}\")\n            continue\n\n        # 3. Segment the sign clip\n        # Sanitize occurrence_label further for use in filenames\n        safe_occurrence_label = re.sub(r'[^a-zA-Z0-9_+-]', '', occurrence_label)\n        segmented_clip_name = f\"sign_{video_id}_{safe_occurrence_label}_{start_frame}-{end_frame}.mp4\"\n        temp_segmented_clip_path = SEGMENTED_VIDEO_DIR / segmented_clip_name\n        \n        final_segmented_clip_path = segment_video_ffmpeg(downloaded_full_video_path, temp_segmented_clip_path, start_frame, end_frame)\n        if not final_segmented_clip_path:\n            print(f\"Skipping due to segmentation error for sign {video_id} from {full_video_filename_excel}\")\n            continue\n            \n        # 4. Perform Pose Estimation on the segmented clip\n        output_npz_filename = f\"poses_{video_id}_{safe_occurrence_label}_{start_frame}-{end_frame}.npz\"\n        output_npz_path = POSE_DATA_OUTPUT_DIR / output_npz_filename\n\n        if output_npz_path.exists():\n            print(f\"Pose data already exists, skipping: {output_npz_path}\")\n            if CLEANUP_TEMP_FILES and final_segmented_clip_path.exists():\n                final_segmented_clip_path.unlink() # Clean up segmented clip\n            continue\n\n        cap = cv2.VideoCapture(str(final_segmented_clip_path))\n        if not cap.isOpened():\n            print(f\"Error: Could not open segmented video {final_segmented_clip_path}\")\n            if CLEANUP_TEMP_FILES and final_segmented_clip_path.exists():\n                final_segmented_clip_path.unlink()\n            continue\n\n        all_frame_poses = []\n        frame_idx = 0\n        total_clip_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        pbar_frames = tqdm(total=total_clip_frames, desc=f\"Frames in {segmented_clip_name}\", leave=False)\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            pose_data = extract_pose_from_frame(frame, detector, pose_estimator)\n            if pose_data is not None:\n                all_frame_poses.append(pose_data)\n            else:\n                num_keypoints = pose_estimator.cfg.data_cfg.num_keypoints if hasattr(pose_estimator.cfg, 'data_cfg') else 133\n                all_frame_poses.append(np.full((num_keypoints, 3), np.nan))\n            \n            frame_idx += 1\n            pbar_frames.update(1)\n        \n        cap.release()\n        pbar_frames.close()\n\n        if all_frame_poses:\n            all_frame_poses_np = np.array(all_frame_poses)\n            np.savez_compressed(output_npz_path, poses=all_frame_poses_np)\n            print(f\"Saved pose data to {output_npz_path} (Shape: {all_frame_poses_np.shape})\")\n        else:\n            print(f\"No poses extracted for segmented clip {final_segmented_clip_path}\")\n\n        # 5. Cleanup temporary segmented clip\n        if CLEANUP_TEMP_FILES and final_segmented_clip_path.exists():\n            print(f\"Cleaning up segmented clip: {final_segmented_clip_path}\")\n            final_segmented_clip_path.unlink()\n            \n    # Optional: Cleanup downloaded full videos after all processing if desired\n    # if CLEANUP_TEMP_FILES:\n    #     for item in FULL_VIDEO_DOWNLOAD_DIR.iterdir():\n    #         if item.is_file(): item.unlink()\n    #     FULL_VIDEO_DOWNLOAD_DIR.rmdir() # if empty\n\nprint(\"\\nAll selected signs processed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:39:02.439482Z","iopub.execute_input":"2025-06-06T14:39:02.440076Z","iopub.status.idle":"2025-06-06T14:39:07.354575Z","shell.execute_reply.started":"2025-06-06T14:39:02.440051Z","shell.execute_reply":"2025-06-06T14:39:07.353795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_SIGNS_TO_PROCESS = 20  # Increase to build a larger dictionary, e.g., 20, 50, 100\nCLEANUP_TEMP_FILES = False # Keep segmented videos for now for easier visual reference\n\nexcel_data_df = load_excel_data(EXCEL_FILE_PATH, num_samples=NUM_SIGNS_TO_PROCESS)\n\n# --- GLOSS TO NPZ MAPPING ---\ngloss_to_npz_map = {}\nGLOSS_MAP_FILE = BASE_WORKING_DIR / 'gloss_to_pose_map.json' # Store as JSON\n\nif excel_data_df.empty:\n    print(\"No data loaded from Excel. Exiting processing loop.\")\nelse:\n    print(f\"Processing {len(excel_data_df)} selected sign entries...\")\n    \n    col_video_id = 'Video ID number'\n    # Choose the best column for the primary gloss. \n    # 'occurrence label' might have variants. 'entry/variant gloss label' or 'Class Label' might be better.\n    # Let's assume 'Class Label' is a good candidate for a cleaned gloss.\n    col_gloss_label = 'Class Label' # Or 'occurrence label', 'entry/variant gloss label'\n    col_start_frame_sign = 'start frame of the sign (relative to full videos)'\n    col_end_frame_sign = 'end frame of the sign (relative to full videos)'\n    col_full_video_file = 'full video file'\n\n    # Ensure the chosen gloss column exists\n    if col_gloss_label not in excel_data_df.columns:\n        print(f\"ERROR: Chosen gloss column '{col_gloss_label}' not found in Excel. Available columns: {excel_data_df.columns.tolist()}\")\n        # Fallback or raise error\n        # For now, let's try 'occurrence label' if 'Class Label' is missing\n        if 'occurrence label' in excel_data_df.columns:\n            col_gloss_label = 'occurrence label'\n            print(f\"Falling back to use '{col_gloss_label}' for gloss information.\")\n        else:\n            raise KeyError(f\"Neither '{col_gloss_label}' nor 'occurrence label' found in Excel columns.\")\n\n\n    for index, row in tqdm(excel_data_df.iterrows(), total=excel_data_df.shape[0], desc=\"Processing Signs\"):\n        video_id = str(row[col_video_id])\n        \n        # Extract and clean gloss\n        raw_gloss = str(row[col_gloss_label])\n        # Basic cleaning: uppercase, remove annotations like (1), +, #. Adapt as needed.\n        # This cleaning needs to be consistent with how your T2G system will output glosses.\n        cleaned_gloss = re.sub(r'\\(\\d+\\)', '', raw_gloss).strip() # Remove (1), (2) etc.\n        cleaned_gloss = cleaned_gloss.replace('+', '').replace('#', '').upper()\n        # If a gloss can have multiple parts separated by '/', decide how to handle:\n        # Option 1: Store each part separately if they represent distinct signs.\n        # Option 2: Keep them together if it's a compound sign that has a single video.\n        # For ASLLVD, often a \"Gloss Variant\" or \"occurrence label\" is one unit.\n        # The example `(1)CHEAT` in 'Class Label' suggests it's often a single unit.\n        # If your T2G produces \"CHEAT\", you want this to match.\n        \n        start_frame = int(row[col_start_frame_sign])\n        end_frame = int(row[col_end_frame_sign])\n        full_video_filename_excel = row[col_full_video_file]\n\n        print(f\"\\nProcessing sign: ID {video_id}, Raw Gloss '{raw_gloss}' -> Cleaned Gloss '{cleaned_gloss}', Source {full_video_filename_excel}, Frames {start_frame}-{end_frame}\")\n\n        video_info_tuple = parse_full_video_filename(full_video_filename_excel)\n        if not all(video_info_tuple):\n            print(f\"Skipping due to filename parse error: {full_video_filename_excel}\")\n            continue\n        \n        downloaded_full_video_path = download_full_video(video_info_tuple, FULL_VIDEO_DOWNLOAD_DIR)\n        if not downloaded_full_video_path:\n            print(f\"Skipping due to download error for: {full_video_filename_excel}\")\n            continue\n\n        # Use cleaned_gloss for a more stable filename component if it's simple enough\n        # Otherwise, stick to video_id and occurrence_label for uniqueness.\n        # Let's use video_id and a safe version of the raw_gloss/occurrence for filename parts\n        # to ensure uniqueness if multiple different signs have the same cleaned_gloss (e.g. homonyms if not distinguished by context)\n        # For the dictionary, we use `cleaned_gloss`.\n        \n        # Filename for segmented clip (using video_id and frames for uniqueness)\n        # Segmented clip name:\n        safe_raw_gloss_for_filename = re.sub(r'[^a-zA-Z0-9_+-]', '', raw_gloss) # Sanitize original label for filename\n        segmented_clip_name = f\"sign_{video_id}_{safe_raw_gloss_for_filename}_{start_frame}-{end_frame}.mp4\"\n        temp_segmented_clip_path = SEGMENTED_VIDEO_DIR / segmented_clip_name\n        \n        final_segmented_clip_path = segment_video_ffmpeg(downloaded_full_video_path, temp_segmented_clip_path, start_frame, end_frame)\n        if not final_segmented_clip_path:\n            print(f\"Skipping due to segmentation error for sign {video_id} from {full_video_filename_excel}\")\n            continue\n            \n        output_npz_filename = f\"poses_{video_id}_{safe_raw_gloss_for_filename}_{start_frame}-{end_frame}.npz\"\n        output_npz_path = POSE_DATA_OUTPUT_DIR / output_npz_filename\n\n        if os.path.exists(output_npz_path):\n            print(f\"Pose data already exists: {output_npz_path}\")\n        else:\n            cap = cv2.VideoCapture(str(final_segmented_clip_path))\n            if not cap.isOpened():\n                print(f\"Error: Could not open segmented video {final_segmented_clip_path}\")\n                if CLEANUP_TEMP_FILES and final_segmented_clip_path.exists():\n                    final_segmented_clip_path.unlink()\n                continue\n\n            all_frame_poses = []\n            frame_idx = 0\n            total_clip_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            \n            pbar_frames = tqdm(total=total_clip_frames, desc=f\"Frames in {segmented_clip_name}\", leave=False)\n            while cap.isOpened():\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                pose_data = extract_pose_from_frame(frame, detector, pose_estimator)\n                if pose_data is not None:\n                    all_frame_poses.append(pose_data)\n                else:\n                    num_keypoints = 133 # Default based on RTMPose wholebody\n                    # Try to get from config if possible, though pose_estimator might not be fully initialized with data_cfg here\n                    try: num_keypoints = pose_estimator.cfg.data_cfg.num_keypoints\n                    except: pass\n                    all_frame_poses.append(np.full((num_keypoints, 3), np.nan))\n                \n                frame_idx += 1\n                pbar_frames.update(1)\n            \n            cap.release()\n            pbar_frames.close()\n\n            if all_frame_poses:\n                all_frame_poses_np = np.array(all_frame_poses)\n                np.savez_compressed(output_npz_path, poses=all_frame_poses_np)\n                print(f\"Saved pose data to {output_npz_path} (Shape: {all_frame_poses_np.shape})\")\n            else:\n                print(f\"No poses extracted for segmented clip {final_segmented_clip_path}\")\n        \n        # --- Add to gloss_to_npz_map ---\n        # Only add if NPZ was created or already existed\n        if os.path.exists(output_npz_path):\n            if cleaned_gloss not in gloss_to_npz_map:\n                gloss_to_npz_map[cleaned_gloss] = []\n            # Store the relative path for portability if this map is used elsewhere\n            relative_npz_path = str(output_npz_path.relative_to(BASE_WORKING_DIR))\n            if relative_npz_path not in gloss_to_npz_map[cleaned_gloss]:\n                 gloss_to_npz_map[cleaned_gloss].append(relative_npz_path)\n            print(f\"Mapped gloss '{cleaned_gloss}' to NPZ: {relative_npz_path}\")\n        else:\n            print(f\"Skipping mapping for gloss '{cleaned_gloss}' as NPZ file was not created: {output_npz_path}\")\n\n\n        if CLEANUP_TEMP_FILES and final_segmented_clip_path.exists() and not os.path.exists(output_npz_path):\n            # Clean up segmented clip only if NPZ was NOT created and we intend to clean up\n            print(f\"Cleaning up failed segmented clip: {final_segmented_clip_path}\")\n            final_segmented_clip_path.unlink()\n            \n# --- Save the gloss_to_npz_map ---\nimport json\nwith open(GLOSS_MAP_FILE, 'w') as f:\n    json.dump(gloss_to_npz_map, f, indent=4)\nprint(f\"\\nGloss to NPZ map saved to: {GLOSS_MAP_FILE}\")\nprint(f\"Total unique glosses mapped: {len(gloss_to_npz_map)}\")\n\n# Optional: Cleanup downloaded full videos\n# if CLEANUP_TEMP_FILES:\n# for item in FULL_VIDEO_DOWNLOAD_DIR.iterdir():\n# if item.is_file(): item.unlink()\n# if FULL_VIDEO_DOWNLOAD_DIR.exists() and not any(FULL_VIDEO_DOWNLOAD_DIR.iterdir()):\n# FULL_VIDEO_DOWNLOAD_DIR.rmdir()\n\nprint(\"\\nAll selected signs processed and mapped.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:50:58.304519Z","iopub.execute_input":"2025-06-06T14:50:58.305336Z","iopub.status.idle":"2025-06-06T14:52:21.110562Z","shell.execute_reply.started":"2025-06-06T14:50:58.305307Z","shell.execute_reply":"2025-06-06T14:52:21.109791Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This cell is to verify the output.\nprint(f\"\\n--- Inspecting Output in {POSE_DATA_OUTPUT_DIR} ---\")\ngenerated_files = [f for f in os.listdir(POSE_DATA_OUTPUT_DIR) if f.endswith('.npz')]\n\nif generated_files:\n    print(f\"Found {len(generated_files)} NPZ files.\")\n    # Load the first generated .npz file as an example\n    example_npz_path = POSE_DATA_OUTPUT_DIR / generated_files[0]\n    print(f\"Loading example NPZ file: {example_npz_path}\")\n    \n    try:\n        data = np.load(example_npz_path)\n        print(\"Keys in the NPZ file:\", data.files) \n        \n        if 'poses' in data:\n            poses_array = data['poses']\n            print(f\"Shape of the 'poses' array: {poses_array.shape}\")\n            \n            if poses_array.shape[0] > 0 and poses_array.shape[1] > 0:\n                print(f\"Example pose data (first keypoint, first frame): {poses_array[0, 0, :]}\")\n            else:\n                print(\"Pose array is empty or has an unexpected shape.\")\n        else:\n            print(\"The key 'poses' was not found in the NPZ file.\")\n        data.close() # Important to close the file\n    except Exception as e:\n        print(f\"Error loading or inspecting NPZ file {example_npz_path}: {e}\")\n        \nelse:\n    print(f\"No .npz files found in {POSE_DATA_OUTPUT_DIR}. Ensure the previous cell ran correctly and processed signs.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:57:43.358406Z","iopub.execute_input":"2025-06-06T14:57:43.358803Z","iopub.status.idle":"2025-06-06T14:57:43.367855Z","shell.execute_reply.started":"2025-06-06T14:57:43.358778Z","shell.execute_reply":"2025-06-06T14:57:43.367157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport torch # For PoseDataSample if needed by visualizer\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# --- Configuration for Visualization ---\n# Pick one of the processed signs to visualize by its NPZ file name\n# Example: poses_5939_MOST_3242-3272.npz\nEXAMPLE_NPZ_FILENAME = \"poses_5939_MOST_3242-3272.npz\" # Change this to an existing NPZ file\n\n# Derive corresponding segmented video filename (adjust logic if your naming differs)\n# Assumes segmented video was like: sign_VIDEOID_LABEL_START-END.mp4\nbase_name_from_npz = EXAMPLE_NPZ_FILENAME.replace('poses_', 'sign_').replace('.npz', '.mp4')\nSEGMENTED_CLIP_TO_VISUALIZE = SEGMENTED_VIDEO_DIR / base_name_from_npz # Path defined in Cell 6\n\nPOSE_DATA_NPZ_PATH = POSE_DATA_OUTPUT_DIR / EXAMPLE_NPZ_FILENAME # Path defined in Cell 6\nVISUALIZED_OUTPUT_DIR = BASE_WORKING_DIR / 'visualized_output'\nVISUALIZED_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nVISUALIZED_VIDEO_PATH = VISUALIZED_OUTPUT_DIR / f\"vis_{base_name_from_npz}\"\n\n# --- Initialize MMPose Visualizer ---\n# The pose_estimator (from Cell 4) holds the visualizer and its dataset_meta\n# We need to ensure the scope is correct for the visualizer if it uses registry components\nif 'pose_estimator' not in locals() or 'detector' not in locals():\n    print(\"Please ensure Cell 4 has been run to initialize pose_estimator and detector.\")\nelse:\n    try:\n        from mmengine.registry import DefaultScope\n        from mmpose.visualization import PoseLocalVisualizer # MMPose v1.x typically uses this\n\n        # It's safer to re-initialize a visualizer instance here if needed,\n        # or ensure the one in pose_estimator is correctly configured.\n        # For MMPose 1.x, PoseLocalVisualizer is common.\n        \n        visualizer = PoseLocalVisualizer(\n            name='visualizer_for_output', # name for the visualizer instance\n            # is_openset=True, # May not be needed, depends on visualizer version\n        )\n        visualizer.set_dataset_meta(\n            dataset_meta=pose_estimator.dataset_meta, \n            skeleton_style='coco_wholebody' # Or 'openpose' etc. based on your model\n        )\n        \n        # You can customize drawing style\n        # visualizer.radius = 3\n        # visualizer.line_width = 2\n        # visualizer.alpha = 0.7 # For semi-transparent overlays\n\n        print(f\"Attempting to visualize: {SEGMENTED_CLIP_TO_VISUALIZE}\")\n        print(f\"Using pose data from: {POSE_DATA_NPZ_PATH}\")\n\n        if not SEGMENTED_CLIP_TO_VISUALIZE.exists():\n            print(f\"ERROR: Segmented video not found: {SEGMENTED_CLIP_TO_VISUALIZE}\")\n            print(\"Ensure 'CLEANUP_TEMP_FILES = False' in Cell 7 and re-run it, or that the file exists.\")\n        elif not POSE_DATA_NPZ_PATH.exists():\n            print(f\"ERROR: Pose NPZ file not found: {POSE_DATA_NPZ_PATH}\")\n        else:\n            # Load pose data\n            pose_data_archive = np.load(POSE_DATA_NPZ_PATH)\n            all_frame_poses_np = pose_data_archive['poses'] # Shape: (num_frames, num_keypoints, 3)\n\n            # Open video capture\n            cap = cv2.VideoCapture(str(SEGMENTED_CLIP_TO_VISUALIZE))\n            if not cap.isOpened():\n                print(f\"Error: Could not open video {SEGMENTED_CLIP_TO_VISUALIZE}\")\n            else:\n                fps = cap.get(cv2.CAP_PROP_FPS)\n                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                \n                # Define video writer\n                fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n                video_writer = cv2.VideoWriter(str(VISUALIZED_VIDEO_PATH), fourcc, fps, (width, height))\n                print(f\"Outputting visualized video to: {VISUALIZED_VIDEO_PATH}\")\n\n                num_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n                num_pose_frames = all_frame_poses_np.shape[0]\n\n                if num_video_frames != num_pose_frames:\n                    print(f\"Warning: Mismatch in frame count! Video has {num_video_frames}, Poses have {num_pose_frames}.\")\n                    print(\"Visualization will proceed up to the shorter duration.\")\n                \n                frames_to_process = min(num_video_frames, num_pose_frames)\n\n                for frame_idx in tqdm(range(frames_to_process), desc=\"Visualizing frames\"):\n                    ret, frame_bgr = cap.read()\n                    if not ret:\n                        break\n\n                    current_pose_keypoints = all_frame_poses_np[frame_idx, :, :2] # (N, K, 2) -> x,y\n                    current_pose_scores = all_frame_poses_np[frame_idx, :, 2]    # (N, K)   -> scores\n                    \n                    # Create a PoseDataSample-like structure for the visualizer\n                    # For MMPose 1.x, visualizer.add_datasample expects a data_sample object\n                    # which has `pred_instances` with `keypoints` and `keypoint_scores`.\n                    # We only have one instance (person) per frame from our saved data.\n                    \n                    # Create a dictionary to mimic PoseDataSample structure for visualization\n                    # This structure might need adjustment based on the specific PoseLocalVisualizer version\n                    drawn_frame = visualizer.draw_instance_pred(\n                        image=frame_bgr.copy(), # Draw on a copy\n                        instances={ # Mimicking structure, may need specific keys\n                            'keypoints': current_pose_keypoints[np.newaxis, ...], # Add batch dim: (1, K, 2)\n                            'keypoint_scores': current_pose_scores[np.newaxis, ...] # Add batch dim: (1, K)\n                        }\n                    )\n                    \n                    video_writer.write(drawn_frame)\n                \n                cap.release()\n                video_writer.release()\n                print(f\"Finished visualizing. Video saved to {VISUALIZED_VIDEO_PATH}\")\n                # You can then download this video from the Kaggle output directory.\n\n    except Exception as e:\n        print(f\"An error occurred during visualization setup or processing: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T14:58:21.607583Z","iopub.execute_input":"2025-06-06T14:58:21.608270Z","iopub.status.idle":"2025-06-06T14:58:21.654550Z","shell.execute_reply.started":"2025-06-06T14:58:21.608242Z","shell.execute_reply":"2025-06-06T14:58:21.653617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport torch # For PoseDataSample if needed by visualizer\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# --- Configuration for Visualization ---\n# Ensure this NPZ file exists from your Cell 7 run\nEXAMPLE_NPZ_FILENAME = \"poses_5939_MOST_3242-3272.npz\" # Change this if needed\n# Or pick another one from your output:\n# EXAMPLE_NPZ_FILENAME = \"poses_615_RUN_2780-2845.npz\" \n\n# Derive corresponding segmented video filename\nbase_name_from_npz = EXAMPLE_NPZ_FILENAME.replace('poses_', 'sign_').replace('.npz', '.mp4')\nSEGMENTED_CLIP_TO_VISUALIZE = SEGMENTED_VIDEO_DIR / base_name_from_npz\nPOSE_DATA_NPZ_PATH = POSE_DATA_OUTPUT_DIR / EXAMPLE_NPZ_FILENAME\nVISUALIZED_OUTPUT_DIR = BASE_WORKING_DIR / 'visualized_output'\nVISUALIZED_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nVISUALIZED_VIDEO_PATH = VISUALIZED_OUTPUT_DIR / f\"vis_{base_name_from_npz}\"\n\n# Check if models are loaded (pose_estimator should have dataset_meta)\nif 'pose_estimator' not in locals():\n    print(\"ERROR: 'pose_estimator' not found. Please ensure Cell 4 (model initialization) has been run successfully.\")\nelse:\n    try:\n        from mmengine.registry import DefaultScope\n        from mmpose.visualization import PoseLocalVisualizer \n        from mmpose.structures import PoseDataSample # For creating the data sample structure\n        from mmengine.structures import InstanceData # For pred_instances\n\n        # Initialize visualizer (or reuse from pose_estimator if configured)\n        # For consistency and explicit control, let's create a new one for this task.\n        visualizer = PoseLocalVisualizer(\n            name='visualizer_for_output_video',\n            radius=3,  # Radius of a keypoint\n            line_width=2,  # Line width of a skeleton\n            # is_openset=True, # May not be needed for drawing known skeletons\n        )\n        visualizer.set_dataset_meta(\n            dataset_meta=pose_estimator.dataset_meta, \n            # Ensure the skeleton style matches your model.\n            # RTMPose-wholebody often uses 'coco_wholebody' or 'openpose' depending on the exact variant.\n            # Check pose_estimator.dataset_meta['skeleton_links'] or similar if unsure.\n            skeleton_style='coco_wholebody' \n        )\n        \n        print(f\"Attempting to visualize: {SEGMENTED_CLIP_TO_VISUALIZE}\")\n        print(f\"Using pose data from: {POSE_DATA_NPZ_PATH}\")\n\n        if not SEGMENTED_CLIP_TO_VISUALIZE.exists():\n            print(f\"ERROR: Segmented video not found: {SEGMENTED_CLIP_TO_VISUALIZE}\")\n            print(\"Ensure 'CLEANUP_TEMP_FILES = False' in Cell 7 and re-run it, or that the file exists.\")\n        elif not POSE_DATA_NPZ_PATH.exists():\n            print(f\"ERROR: Pose NPZ file not found: {POSE_DATA_NPZ_PATH}\")\n        else:\n            pose_data_archive = np.load(POSE_DATA_NPZ_PATH)\n            all_frame_poses_np = pose_data_archive['poses'] \n\n            cap = cv2.VideoCapture(str(SEGMENTED_CLIP_TO_VISUALIZE))\n            if not cap.isOpened():\n                print(f\"Error: Could not open video {SEGMENTED_CLIP_TO_VISUALIZE}\")\n            else:\n                fps = cap.get(cv2.CAP_PROP_FPS)\n                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                \n                fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n                video_writer = cv2.VideoWriter(str(VISUALIZED_VIDEO_PATH), fourcc, fps, (width, height))\n                print(f\"Outputting visualized video to: {VISUALIZED_VIDEO_PATH}\")\n\n                num_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n                num_pose_frames = all_frame_poses_np.shape[0]\n\n                if num_video_frames != num_pose_frames:\n                    print(f\"Warning: Mismatch in frame count! Video has {num_video_frames}, Poses have {num_pose_frames}.\")\n                \n                frames_to_process = min(num_video_frames, num_pose_frames)\n\n                for frame_idx in tqdm(range(frames_to_process), desc=\"Visualizing frames\"):\n                    ret, frame_bgr = cap.read()\n                    if not ret:\n                        break\n\n                    current_pose_keypoints = all_frame_poses_np[frame_idx, :, :2] # (K, 2) -> x,y\n                    current_pose_scores = all_frame_poses_np[frame_idx, :, 2]    # (K)   -> scores\n                    \n                    # Create a PoseDataSample structure for the visualizer\n                    data_sample = PoseDataSample()\n                    pred_instances = InstanceData()\n                    pred_instances.keypoints = current_pose_keypoints[np.newaxis, ...] # Shape: (1, K, 2)\n                    pred_instances.keypoint_scores = current_pose_scores[np.newaxis, ...] # Shape: (1, K)\n                    data_sample.pred_instances = pred_instances\n                    \n                    # Add metainfo if required by visualizer (e.g., image path for some visualizers, not always needed for drawing)\n                    # data_sample.set_metainfo({'img_path': 'dummy_path_for_visualization'})\n\n\n                    # Use add_datasample and get_image\n                    # The visualizer draws on an internal canvas or the provided image.\n                    # We provide the current frame_bgr to draw upon.\n                    with DefaultScope.overwrite_default_scope(scope_name='mmpose'): # Ensure correct scope for visualizer internals\n                        visualizer.add_datasample(\n                            name='frame_visualization', # A name for this drawing operation\n                            image=frame_bgr.copy(), # Draw on a copy of the current frame\n                            data_sample=data_sample,\n                            draw_gt=False, # We don't have ground truth here\n                            draw_heatmap=False,\n                            draw_bbox=False, # We are providing keypoints directly\n                            # show=False, # Don't show interactively\n                            # wait_time=0,\n                            # out_file=None # We are getting the image to write to video\n                        )\n                        drawn_frame = visualizer.get_image()\n                    \n                    video_writer.write(drawn_frame)\n                \n                cap.release()\n                video_writer.release()\n                print(f\"Finished visualizing. Video saved to {VISUALIZED_VIDEO_PATH}\")\n\n    except Exception as e:\n        print(f\"An error occurred during visualization setup or processing: {e}\")\n        import traceback\n        traceback.print_exc()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T15:04:30.217951Z","iopub.execute_input":"2025-06-06T15:04:30.218287Z","iopub.status.idle":"2025-06-06T15:04:30.450623Z","shell.execute_reply.started":"2025-06-06T15:04:30.218267Z","shell.execute_reply":"2025-06-06T15:04:30.449845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # Temporary Cell (e.g., Cell 4.1) to inspect skeleton_links\n\n    if 'pose_estimator' in locals():\n        print(\"--- pose_estimator.dataset_meta ---\")\n        # Print the whole dataset_meta to see what's available\n        # print(pose_estimator.dataset_meta) \n        \n        print(\"\\n--- Keypoint Names (first 20 for brevity) ---\")\n        if 'keypoint_names' in pose_estimator.dataset_meta:\n            print(pose_estimator.dataset_meta['keypoint_names'][:20])\n        else:\n            print(\"'keypoint_names' not found in dataset_meta.\")\n\n        print(\"\\n--- Skeleton Links ---\")\n        if 'skeleton_links' in pose_estimator.dataset_meta:\n            actual_skeleton_links = pose_estimator.dataset_meta['skeleton_links']\n            print(f\"Found {len(actual_skeleton_links)} skeleton links.\")\n            print(\"First 128 links:\", actual_skeleton_links[:128])\n            # You can assign this to a variable to copy-paste into Cell 10\n            # For example:\n            # my_links_for_cell_10 = actual_skeleton_links \n        else:\n            print(\"'skeleton_links' not found in dataset_meta. This is unexpected for visualization.\")\n            print(\"Available keys in dataset_meta:\", pose_estimator.dataset_meta.keys())\n\n    else:\n        print(\"Variable 'pose_estimator' is not defined. Please run Cell 4 first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T15:28:33.690255Z","iopub.execute_input":"2025-06-06T15:28:33.690538Z","iopub.status.idle":"2025-06-06T15:28:33.697446Z","shell.execute_reply.started":"2025-06-06T15:28:33.690517Z","shell.execute_reply":"2025-06-06T15:28:33.696605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML # For displaying the animation in Jupyter/Kaggle\nfrom pathlib import Path\nimport os # For os.listdir\n\n# --- Configuration ---\n# Ensure EXAMPLE_NPZ_FILENAME is defined from Cell 9 or set it here\nif 'EXAMPLE_NPZ_FILENAME' not in locals() or not EXAMPLE_NPZ_FILENAME:\n    print(\"Warning: EXAMPLE_NPZ_FILENAME not set, picking one from output if available.\")\n    npz_files_in_output = [f for f in os.listdir(POSE_DATA_OUTPUT_DIR) if f.endswith('.npz')]\n    if npz_files_in_output:\n        EXAMPLE_NPZ_FILENAME = npz_files_in_output[0]\n        print(f\"Using NPZ file for animation: {EXAMPLE_NPZ_FILENAME}\")\n    else:\n        print(\"ERROR: No NPZ files found in output directory. Cannot create animation.\")\n        raise FileNotFoundError(\"No NPZ files available for animation.\")\n\nNPZ_TO_ANIMATE = POSE_DATA_OUTPUT_DIR / EXAMPLE_NPZ_FILENAME\nANIMATION_OUTPUT_DIR = BASE_WORKING_DIR / 'animations'\nANIMATION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nANIMATION_FILENAME = ANIMATION_OUTPUT_DIR / f\"anim_{NPZ_TO_ANIMATE.stem}.mp4\"\n\n# --- Load Pose Data ---\nif not NPZ_TO_ANIMATE.exists():\n    print(f\"ERROR: NPZ file not found: {NPZ_TO_ANIMATE}\")\nelse:\n    print(f\"Loading pose data from: {NPZ_TO_ANIMATE}\")\n    pose_data_archive = np.load(NPZ_TO_ANIMATE)\n    poses_array = pose_data_archive['poses']\n    num_frames, num_keypoints_in_data, _ = poses_array.shape\n    print(f\"Loaded {num_frames} frames with {num_keypoints_in_data} keypoints each.\")\n\n    # --- Define Skeleton Connections (Using the 128 links from your dataset_meta) ---\n    actual_skeleton_links = [\n        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], \n        [5, 6], [5, 7], [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], \n        [1, 3], [2, 4], [3, 5], [4, 6], [17, 18], [18, 19], [19, 20], \n        [20, 21], [21, 22], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28], \n        [28, 29], [23, 30], [30, 31], [31, 32], [32, 33], [23, 34], [34, 35], \n        [35, 36], [36, 37], [23, 38], [38, 39], [39, 40], [40, 41], [42, 43], \n        [43, 44], [44, 45], [45, 46], [42, 47], [47, 48], [48, 49], [49, 50], \n        [42, 51], [51, 52], [52, 53], [53, 54], [42, 55], [55, 56], [56, 57], \n        [57, 58], [59, 60], [60, 61], [61, 62], [62, 63], [59, 64], [64, 65], \n        [65, 66], [66, 67], [59, 68], [68, 69], [69, 70], [70, 71], [59, 72], \n        [72, 73], [73, 74], [74, 75], [59, 76], [76, 77], [77, 78], [78, 79], \n        [80, 81], [81, 82], [82, 83], [80, 84], [84, 85], [85, 86], [80, 87], \n        [87, 88], [88, 89], [80, 90], [91, 92], [92, 93], [93, 94], [94, 95], \n        [91, 96], [96, 97], [97, 98], [98, 99], [91, 100], [100, 101], \n        [101, 102], [102, 103], [91, 104], [104, 105], [105, 106], [106, 107], \n        [91, 108], [108, 109], [109, 110], [110, 111], [112, 113], \n        [113, 114], [114, 115], [115, 116], [112, 117], [117, 118], \n        [118, 119], [119, 120], [112, 121], [121, 122], [122, 123], [123, 124], \n        [112, 125], [125, 126], [126, 127], [127, 128], [112, 129], \n        [129, 130], [130, 131], [131, 132]\n    ]\n    print(f\"Using {len(actual_skeleton_links)} skeleton links for animation.\")\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n    lines = [ax.plot([], [], 'o-', color='cyan', markersize=1, linewidth=0.75)[0] for _ in actual_skeleton_links] \n    \n    valid_poses = poses_array[~np.isnan(poses_array).any(axis=(1,2))]\n    if valid_poses.shape[0] > 0:\n        all_x = valid_poses[:, :, 0].flatten()\n        all_y = valid_poses[:, :, 1].flatten()\n        all_x_no_nan = all_x[~np.isnan(all_x)]\n        all_y_no_nan = all_y[~np.isnan(all_y)]\n\n        if all_x_no_nan.size > 0 and all_y_no_nan.size > 0:\n            padding = 50 \n            x_min, x_max = np.min(all_x_no_nan) - padding, np.max(all_x_no_nan) + padding\n            y_min, y_max = np.min(all_y_no_nan) - padding, np.max(all_y_no_nan) + padding\n            ax.set_xlim(x_min, x_max)\n            ax.set_ylim(y_max, y_min) \n        else:\n            print(\"Warning: No valid (non-NaN) keypoints found to determine plot limits. Using defaults.\")\n            ax.set_xlim(0, 640) \n            ax.set_ylim(480, 0)\n    else:\n        print(\"Warning: No frames with valid poses found to determine plot limits. Using defaults.\")\n        ax.set_xlim(0, 640) \n        ax.set_ylim(480, 0)\n        \n    ax.set_aspect('equal', adjustable='box')\n    ax.set_facecolor('black') \n    fig.patch.set_facecolor('black') \n    ax.tick_params(axis='x', colors='white') \n    ax.tick_params(axis='y', colors='white') \n    ax.spines['bottom'].set_color('white') \n    ax.spines['top'].set_color('white')   \n    ax.spines['right'].set_color('white')\n    ax.spines['left'].set_color('white')\n    plt.title(f\"2D Pose Animation: {NPZ_TO_ANIMATE.name}\", color='white')\n\n    confidence_threshold = 0.3 \n\n    def init_animation():\n        for line in lines:\n            line.set_data([], [])\n        return lines\n\n    def update_animation(frame_idx):\n        keypoints_frame = poses_array[frame_idx, :, :2] \n        scores_frame = poses_array[frame_idx, :, 2]     \n\n        for i, (idx1, idx2) in enumerate(actual_skeleton_links):\n            if idx1 < num_keypoints_in_data and idx2 < num_keypoints_in_data:\n                is_valid_p1 = not np.isnan(keypoints_frame[idx1, 0]) and not np.isnan(keypoints_frame[idx1, 1])\n                is_valid_p2 = not np.isnan(keypoints_frame[idx2, 0]) and not np.isnan(keypoints_frame[idx2, 1])\n\n                if is_valid_p1 and is_valid_p2 and \\\n                   scores_frame[idx1] > confidence_threshold and \\\n                   scores_frame[idx2] > confidence_threshold:\n                    x_coords = [keypoints_frame[idx1, 0], keypoints_frame[idx2, 0]]\n                    y_coords = [keypoints_frame[idx1, 1], keypoints_frame[idx2, 1]]\n                    lines[i].set_data(x_coords, y_coords)\n                    lines[i].set_visible(True)\n                else:\n                    lines[i].set_data([], []) \n                    lines[i].set_visible(False) \n            else:\n                 lines[i].set_data([], []) \n                 lines[i].set_visible(False) \n        return lines\n\n    ani_fps = 30 \n    ani = animation.FuncAnimation(fig, update_animation, frames=num_frames,\n                                  init_func=init_animation, blit=True, interval=int(1000/ani_fps))\n\n    print(f\"Saving animation to {ANIMATION_FILENAME}...\")\n    try:\n        ani.save(ANIMATION_FILENAME, writer='ffmpeg', fps=ani_fps, dpi=150, \n                 progress_callback=lambda current_frame, total_frames: print(f\"Saving frame {current_frame+1}/{total_frames}\", end='\\r') if total_frames and (current_frame + 1) % 10 == 0 or current_frame + 1 == total_frames else None)\n        print(\"\\nAnimation saved.\")\n        plt.close(fig) \n        # print(\"Preparing HTML for display (this might take a moment)...\")\n        # display_html = HTML(ani.to_jshtml(fps=ani_fps))\n        # display(display_html)\n        # plt.close(fig)\n        print(f\"To view, download: {ANIMATION_FILENAME} from the 'animations' directory in your Kaggle output.\")\n    except Exception as e:\n        print(f\"\\nError saving animation: {e}\")\n        import traceback\n        traceback.print_exc()\n        plt.close(fig)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T15:30:39.269274Z","iopub.execute_input":"2025-06-06T15:30:39.269607Z","iopub.status.idle":"2025-06-06T15:30:43.274367Z","shell.execute_reply.started":"2025-06-06T15:30:39.269581Z","shell.execute_reply":"2025-06-06T15:30:43.273625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nimport os\nfrom IPython.display import HTML, display\nimport json\n\n# --- Configuration ---\nPOSE_DATA_DIR = \"/kaggle/working/pose_data/\"\nOUTPUT_ANIM_DIR = \"/kaggle/working/animated_skeletons/\"\nGLOSS_TO_POSE_MAP_FILE = \"/kaggle/working/gloss_to_pose_map.json\"\nKEYPOINT_CONFIDENCE_THRESHOLD = 0.3\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_ANIM_DIR, exist_ok=True)\n\n# Load the gloss_to_pose_map\ntry:\n    with open(GLOSS_TO_POSE_MAP_FILE, 'r') as f:\n        gloss_to_pose_map = json.load(f)\n    print(f\"Successfully loaded gloss_to_pose_map from {GLOSS_TO_POSE_MAP_FILE}\")\n    NUM_ANIMATIONS_TO_GENERATE = len(gloss_to_pose_map) # Process all loaded glosses\n    print(f\"Set to generate {NUM_ANIMATIONS_TO_GENERATE} animations.\")\nexcept FileNotFoundError:\n    print(f\"ERROR: Could not find {GLOSS_TO_POSE_MAP_FILE}. Make sure Cell 7 ran correctly.\")\n    gloss_to_pose_map = {}\n    NUM_ANIMATIONS_TO_GENERATE = 0\nexcept json.JSONDecodeError:\n    print(f\"ERROR: Could not decode {GLOSS_TO_POSE_MAP_FILE}. Check its content.\")\n    gloss_to_pose_map = {}\n    NUM_ANIMATIONS_TO_GENERATE = 0\n\n\n# Get skeleton links\nif 'pose_estimator' in globals() and hasattr(pose_estimator, 'dataset_meta') and 'skeleton_links' in pose_estimator.dataset_meta:\n    actual_skeleton_links = pose_estimator.dataset_meta['skeleton_links']\n    print(f\"Using {len(actual_skeleton_links)} skeleton links from pose_estimator.dataset_meta.\")\nelse:\n    print(\"Warning: pose_estimator or its dataset_meta not found. Using a default COCO-WholeBody skeleton definition.\")\n    COCO_WHOLEBODY_SKELETON_LINKS = [\n        [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7], [6, 8], [7, 9], [8, 10],\n        [1, 2], [0, 1], [0, 2], [1, 3], [2, 4], [3, 5], [4, 6], [17, 18], [18, 19], [19, 20], [20, 21], [21, 22],\n        [23, 24], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29], [23, 30], [30, 31], [31, 32], [32, 33], [23, 34],\n        [34, 35], [35, 36], [36, 37], [23, 38], [38, 39], [39, 40], [40, 41], [42, 43], [43, 44], [44, 45], [45, 46],\n        [42, 47], [47, 48], [48, 49], [49, 50], [42, 51], [51, 52], [52, 53], [53, 54], [42, 55], [55, 56], [56, 57],\n        [57, 58], [59, 60], [60, 61], [61, 62], [62, 63], [59, 64], [64, 65], [65, 66], [66, 67], [59, 68], [68, 69],\n        [69, 70], [70, 71], [59, 72], [72, 73], [73, 74], [74, 75], [59, 76], [76, 77], [77, 78], [78, 79], [80, 81],\n        [81, 82], [82, 83], [80, 84], [84, 85], [85, 86], [80, 87], [87, 88], [88, 89], [80, 90], [91, 92], [92, 93],\n        [93, 94], [94, 95], [91, 96], [96, 97], [97, 98], [98, 99], [91, 100], [100, 101], [101, 102], [102, 103],\n        [91, 104], [104, 105], [105, 106], [106, 107], [91, 108], [108, 109], [109, 110], [110, 111], [112, 113],\n        [113, 114], [114, 115], [115, 116], [112, 117], [117, 118], [118, 119], [119, 120], [112, 121], [121, 122],\n        [122, 123], [123, 124], [112, 125], [125, 126], [126, 127], [127, 128], [112, 129], [129, 130], [130, 131],\n        [131, 132]]\n    actual_skeleton_links = COCO_WHOLEBODY_SKELETON_LINKS\n    print(f\"Using a default COCO-WholeBody skeleton definition with {len(actual_skeleton_links)} links.\")\n\nprocessed_glosses_count = 0\nfor gloss_idx, (gloss, path_value) in enumerate(gloss_to_pose_map.items()):\n    if processed_glosses_count >= NUM_ANIMATIONS_TO_GENERATE:\n        break\n    \n    current_npz_path_str = None\n    if isinstance(path_value, list):\n        if len(path_value) > 0 and isinstance(path_value[0], str):\n            current_npz_path_str = path_value[0]\n        else:\n            print(f\"ERROR: Gloss '{gloss}' has a list value, but it's empty or doesn't contain a string. Value: {path_value}. Skipping.\")\n            continue\n    elif isinstance(path_value, str):\n        current_npz_path_str = path_value\n    else:\n        print(f\"ERROR: Path value for gloss '{gloss}' is not a string or list of strings. Type: {type(path_value)}. Value: {path_value}. Skipping.\")\n        continue\n\n    if not current_npz_path_str:\n        print(f\"ERROR: Could not determine NPZ path for gloss '{gloss}'. Skipping.\")\n        continue\n\n    if \"pose_data/\" in current_npz_path_str:\n         base_working_dir = \"/kaggle/working/\"\n         npz_file_path = os.path.join(base_working_dir, current_npz_path_str)\n    else:\n         npz_file_path = os.path.join(POSE_DATA_DIR, current_npz_path_str)\n    \n    if not os.path.exists(npz_file_path):\n        alternative_path = os.path.join(POSE_DATA_DIR, os.path.basename(current_npz_path_str))\n        if os.path.exists(alternative_path):\n            npz_file_path = alternative_path\n        else:\n            print(f\"Could not find NPZ file for gloss '{gloss}'. Checked: '{npz_file_path}' and '{alternative_path}'. Skipping.\")\n            continue\n\n    try:\n        all_poses_npz = np.load(npz_file_path)\n    except Exception as e:\n        print(f\"Error loading NPZ file {npz_file_path}: {e}. Skipping.\")\n        continue\n\n    if 'poses' not in all_poses_npz:\n        print(f\"Key 'poses' not found in {npz_file_path}. Skipping.\")\n        continue\n\n    animation_poses = all_poses_npz['poses']\n    num_frames, num_keypoints, _ = animation_poses.shape\n\n    if num_frames == 0 or num_keypoints == 0:\n        print(f\"No frames or keypoints in {npz_file_path} for gloss '{gloss}'. Skipping.\")\n        continue\n\n    print(f\"\\nGenerating animation for gloss: '{gloss}' (Frames: {num_frames}, Keypoints: {num_keypoints}) from {npz_file_path}\")\n\n    fig, ax = plt.subplots()\n    \n    # Filter poses by confidence for calculating bounds\n    confident_poses = animation_poses[animation_poses[:, :, 2] > KEYPOINT_CONFIDENCE_THRESHOLD]\n    \n    if confident_poses.shape[0] == 0: # Check if any confident keypoints exist across all frames\n        print(f\"No keypoints above confidence threshold for gloss '{gloss}' across all frames. Skipping animation.\")\n        plt.close(fig)\n        continue\n\n    all_x_coords = confident_poses[:, 0]\n    all_y_coords = confident_poses[:, 1]\n    \n    global_min_x, global_max_x = np.min(all_x_coords), np.max(all_x_coords)\n    global_min_y, global_max_y = np.min(all_y_coords), np.max(all_y_coords)\n    padding = 20 \n    \n    # DEBUG: Print axis limits\n    print(f\"  Axis limits for '{gloss}': X=({global_min_x - padding:.2f}, {global_max_x + padding:.2f}), Y=({global_min_y - padding:.2f}, {global_max_y + padding:.2f})\")\n\n    # --- DEBUG: Save first frame as static image ---\n    # (Only for the very first animation being processed for quicker debugging)\n    save_static_frame_debug = (processed_glosses_count == 0) \n\n\n    def update(i):\n        ax.cla() # Clear axis\n        frame_poses = animation_poses[i]\n        \n        ax.set_xlim(global_min_x - padding, global_max_x + padding)\n        ax.set_ylim(global_min_y - padding, global_max_y + padding)\n        ax.set_aspect('equal', adjustable='box')\n        ax.set_title(f\"Gloss: {gloss} - Frame {i+1}/{num_frames}\")\n        ax.invert_yaxis()\n\n        # --- DEBUG: Print some keypoint data for the first frame of the first animation ---\n        if i == 0 and save_static_frame_debug:\n            print(f\"    Frame 0 Keypoints (first 5 for '{gloss}'):\")\n            for kp_idx_print in range(min(5, num_keypoints)):\n                x_p, y_p, conf_p = frame_poses[kp_idx_print]\n                if conf_p > KEYPOINT_CONFIDENCE_THRESHOLD:\n                    print(f\"      KP{kp_idx_print}: ({x_p:.2f}, {y_p:.2f}), Conf: {conf_p:.2f}\")\n\n        # Draw skeleton links\n        for kp_idx1, kp_idx2 in actual_skeleton_links:\n            if kp_idx1 < num_keypoints and kp_idx2 < num_keypoints: # Bounds check\n                x1, y1, conf1 = frame_poses[kp_idx1]\n                x2, y2, conf2 = frame_poses[kp_idx2]\n                if conf1 > KEYPOINT_CONFIDENCE_THRESHOLD and conf2 > KEYPOINT_CONFIDENCE_THRESHOLD:\n                    ax.plot([x1, x2], [y1, y2], color='red', linewidth=2.5, zorder=1)\n        \n        # Draw keypoints (joints)\n        visible_kps_x = []\n        visible_kps_y = []\n        for kp_idx in range(num_keypoints):\n            x, y, conf = frame_poses[kp_idx]\n            if conf > KEYPOINT_CONFIDENCE_THRESHOLD:\n                visible_kps_x.append(x)\n                visible_kps_y.append(y)\n        \n        if visible_kps_x: # Check if there are any keypoints to draw\n            ax.scatter(visible_kps_x, visible_kps_y, c='blue', s=30, zorder=2)\n\n        # --- DEBUG: Save first frame as static image ---\n        if i == 0 and save_static_frame_debug:\n            debug_frame_path = os.path.join(OUTPUT_ANIM_DIR, f\"debug_frame_{gloss_idx}.png\")\n            try:\n                plt.savefig(debug_frame_path)\n                print(f\"    Saved debug frame to {debug_frame_path}\")\n            except Exception as e_fig:\n                print(f\"    Error saving debug frame: {e_fig}\")\n        \n        return [ax] # For blit=True, though we set it to False\n\n    # Create animation, set blit=False for debugging\n    ani = animation.FuncAnimation(fig, update, frames=num_frames, blit=False, interval=50) \n    \n    safe_gloss_name = \"\".join(c if c.isalnum() else \"_\" for c in gloss)\n    output_path = os.path.join(OUTPUT_ANIM_DIR, f\"anim_{safe_gloss_name}.mp4\")\n    \n    try:\n        ani.save(output_path, writer='ffmpeg', fps=20)\n        print(f\"  Saved animation to {output_path}\")\n    except Exception as e:\n        print(f\"  Error saving animation for gloss '{gloss}': {e}\")\n    finally:\n        plt.close(fig) # Ensure figure is closed\n\n    processed_glosses_count += 1\n\nif processed_glosses_count == 0:\n    print(\"\\nNo animations were generated. Check NPZ files, paths, and confidence thresholds.\")\nelse:\n    print(f\"\\nFinished generating {processed_glosses_count} animations in {OUTPUT_ANIM_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T15:56:57.566181Z","iopub.execute_input":"2025-06-06T15:56:57.566517Z","iopub.status.idle":"2025-06-06T15:58:22.278355Z","shell.execute_reply.started":"2025-06-06T15:56:57.566494Z","shell.execute_reply":"2025-06-06T15:58:22.277587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: 2D Sprite Character Animation with Pygame (More Complete)\nimport pygame\nimport numpy as np\nimport math\nimport os\nimport json\nfrom PIL import Image # For saving pygame surface to image file\nimport subprocess # For running ffmpeg\n\n# --- Configuration ---\nPOSE_DATA_DIR = \"/kaggle/working/pose_data/\"\nPYGAME_ANIM_DIR = \"/kaggle/working/pygame_animations/\"\nPYGAME_FRAMES_DIR = \"/kaggle/working/pygame_frames_temp/\" # Temp storage for frames for current video\nGLOSS_TO_POSE_MAP_FILE = \"/kaggle/working/gloss_to_pose_map.json\"\nKEYPOINT_CONFIDENCE_THRESHOLD = 0.3\nNUM_PYGAME_ANIMATIONS_TO_GENERATE = 1 # Generate for one sign first for testing\n\n# Colors\nBG_COLOR = (255, 255, 255) # White background\nLIMB_COLOR = (50, 150, 255, 200) # RGBA, A for alpha (semi-transparent)\nHEAD_COLOR = (255, 200, 150, 200) # RGBA\nTORSO_COLOR = (100, 100, 100, 200) # RGBA\nJOINT_COLOR = (255, 0, 0) # For drawing debug joints\n\n# Sprite dimensions (placeholders)\nLIMB_WIDTH = 20     # Width of the limb/torso rectangles\nHEAD_RADIUS = 25\nJOINT_RADIUS = 5 # For debug drawing\n\n# Screen/Surface padding\nPADDING = 50\n\n# Ensure output directories exist\nos.makedirs(PYGAME_ANIM_DIR, exist_ok=True)\n# For PYGAME_FRAMES_DIR, we'll clear it for each new video\n# os.makedirs(PYGAME_FRAMES_DIR, exist_ok=True)\n\n# --- Pygame Initialization ---\n# Check if Pygame display has been initialized to avoid re-init issues in notebooks\nif not pygame.display.get_init():\n    os.environ['SDL_VIDEODRIVER'] = 'dummy' # Use dummy driver for headless environments\n    pygame.init()\n    print(\"Pygame initialized.\")\nelse:\n    print(\"Pygame already initialized.\")\n\n\n# --- Keypoint Indices (COCO WholeBody - 133 keypoints) ---\nKP_NOSE = 0; KP_L_EYE = 1; KP_R_EYE = 2; KP_L_EAR = 3; KP_R_EAR = 4\nKP_L_SHOULDER = 5; KP_R_SHOULDER = 6; KP_L_ELBOW = 7; KP_R_ELBOW = 8\nKP_L_WRIST = 9; KP_R_WRIST = 10; KP_L_HIP = 11; KP_R_HIP = 12\nKP_L_KNEE = 13; KP_R_KNEE = 14; KP_L_ANKLE = 15; KP_R_ANKLE = 16\n\n# --- Helper Functions ---\ndef calculate_angle_degrees(p1x, p1y, p2x, p2y):\n    return math.degrees(math.atan2(p2y - p1y, p2x - p1x))\n\ndef calculate_distance(p1x, p1y, p2x, p2y):\n    return math.sqrt((p2x - p1x)**2 + (p2y - p1y)**2)\n\ndef draw_limb_sprite(surface, p1_data, p2_data, limb_color, limb_width):\n    \"\"\"Draws a limb between two keypoints p1 and p2 with specified width and color.\"\"\"\n    x1, y1, conf1 = p1_data\n    x2, y2, conf2 = p2_data\n\n    if conf1 < KEYPOINT_CONFIDENCE_THRESHOLD or conf2 < KEYPOINT_CONFIDENCE_THRESHOLD:\n        return\n\n    length = calculate_distance(x1, y1, x2, y2)\n    if length < 1: \n        return\n\n    limb_surface = pygame.Surface((int(length), int(limb_width)), pygame.SRCALPHA) # Ensure int dimensions\n    limb_surface.fill(limb_color)\n\n    angle = calculate_angle_degrees(x1, y1, x2, y2)\n    rotated_limb_surface = pygame.transform.rotate(limb_surface, -angle)\n    \n    rad_angle = math.radians(angle)\n    center_x = x1 + (length / 2.0) * math.cos(rad_angle)\n    center_y = y1 + (length / 2.0) * math.sin(rad_angle)\n    \n    blit_rect = rotated_limb_surface.get_rect(center=(int(center_x), int(center_y))) # Ensure int center\n    surface.blit(rotated_limb_surface, blit_rect.topleft)\n\n\ndef draw_head_sprite(surface, nose_kp, l_shoulder_kp, r_shoulder_kp, head_color, radius):\n    \"\"\"Draws a head, positioned relative to nose and shoulders.\"\"\"\n    nx, ny, nconf = nose_kp\n    lsx, lsy, lsconf = l_shoulder_kp\n    rsx, rsy, rsconf = r_shoulder_kp\n\n    if nconf < KEYPOINT_CONFIDENCE_THRESHOLD :\n        return\n\n    head_center_x = nx\n    head_center_y = ny - radius * 0.5 \n    \n    if lsconf > KEYPOINT_CONFIDENCE_THRESHOLD and rsconf > KEYPOINT_CONFIDENCE_THRESHOLD:\n        shoulder_mid_y = (lsy + rsy) / 2\n        if ny > shoulder_mid_y :\n             head_center_y = shoulder_mid_y - radius \n\n    pygame.draw.circle(surface, head_color, (int(head_center_x), int(head_center_y)), int(radius))\n\n\ndef draw_torso_sprite(surface, sl_kp, sr_kp, hl_kp, hr_kp, torso_color):\n    \"\"\"Draws a torso as a polygon given shoulder and hip keypoints.\"\"\"\n    points_data = [sl_kp, sr_kp, hr_kp, hl_kp] \n    polygon_points = []\n    for p_data in points_data:\n        if p_data[2] > KEYPOINT_CONFIDENCE_THRESHOLD:\n            x_coord = int(p_data[0]) # Ensure integer coordinates\n            y_coord = int(p_data[1]) # Ensure integer coordinates\n            polygon_points.append((x_coord, y_coord))\n        else: \n            return \n            \n    if len(polygon_points) == 4: \n        try:\n            pygame.draw.polygon(surface, torso_color, polygon_points)\n        except TypeError as e:\n            print(f\"ERROR in pygame.draw.polygon: {e}\")\n            print(f\"  Torso color: {torso_color}, type: {type(torso_color)}\")\n            print(f\"  Polygon points: {polygon_points}\")\n            for i_pt, pt_val in enumerate(polygon_points):\n                print(f\"    Point {i_pt}: {pt_val}, type x: {type(pt_val[0])}, type y: {type(pt_val[1])}\")\n            # raise # Optionally re-raise after debugging\n    # else:\n    #     print(f\"DEBUG: Torso not drawn, {len(polygon_points)} valid points.\")\n\n\n# --- Load Data ---\ntry:\n    with open(GLOSS_TO_POSE_MAP_FILE, 'r') as f:\n        gloss_to_pose_map = json.load(f)\n    print(f\"Successfully loaded gloss_to_pose_map from {GLOSS_TO_POSE_MAP_FILE}\")\nexcept Exception as e:\n    print(f\"Error loading gloss_to_pose_map.json: {e}\")\n    gloss_to_pose_map = {}\n\nif not gloss_to_pose_map:\n    print(\"Gloss map is empty. Cannot proceed.\")\nelse:\n    processed_animations_count = 0\n    for gloss, path_values in gloss_to_pose_map.items():\n        if processed_animations_count >= NUM_PYGAME_ANIMATIONS_TO_GENERATE:\n            break\n\n        print(f\"\\nProcessing gloss: {gloss}\")\n        \n        current_frames_dir = os.path.join(PYGAME_FRAMES_DIR, gloss.replace('/', '_')) # Subdirectory for each gloss\n        if os.path.exists(current_frames_dir):\n            for f_name in os.listdir(current_frames_dir):\n                os.remove(os.path.join(current_frames_dir, f_name))\n        else:\n            os.makedirs(current_frames_dir)\n\n        current_npz_path_str = None\n        if isinstance(path_values, list) and path_values:\n            current_npz_path_str = path_values[0]\n        elif isinstance(path_values, str):\n            current_npz_path_str = path_values\n        else:\n            print(f\"  Invalid path value for gloss '{gloss}': {path_values}. Skipping.\")\n            continue\n\n        if \"pose_data/\" in current_npz_path_str:\n             npz_file_path = os.path.join(\"/kaggle/working/\", current_npz_path_str)\n        else:\n             npz_file_path = os.path.join(POSE_DATA_DIR, current_npz_path_str)\n\n        if not os.path.exists(npz_file_path):\n            print(f\"  NPZ file not found: {npz_file_path}. Skipping.\")\n            continue\n        \n        try:\n            animation_poses_all_data = np.load(npz_file_path)\n            animation_poses = animation_poses_all_data['poses'] \n        except Exception as e:\n            print(f\"  Error loading NPZ {npz_file_path}: {e}. Skipping.\")\n            continue\n\n        num_frames, num_keypoints, _ = animation_poses.shape\n        if num_frames == 0:\n            print(f\"  No frames in {npz_file_path}. Skipping.\")\n            continue\n        \n        confident_coords_x = []\n        confident_coords_y = []\n        for frame_idx in range(num_frames):\n            for kp_idx in range(num_keypoints):\n                if animation_poses[frame_idx, kp_idx, 2] > KEYPOINT_CONFIDENCE_THRESHOLD:\n                    confident_coords_x.append(animation_poses[frame_idx, kp_idx, 0])\n                    confident_coords_y.append(animation_poses[frame_idx, kp_idx, 1])\n        \n        if not confident_coords_x or not confident_coords_y:\n            print(f\"  No confident keypoints found for gloss '{gloss}' to determine bounds. Skipping.\")\n            continue\n\n        min_x, max_x = min(confident_coords_x), max(confident_coords_x)\n        min_y, max_y = min(confident_coords_y), max(confident_coords_y)\n\n        surface_w_raw = int(max_x - min_x + 2 * PADDING)\n        surface_h_raw = int(max_y - min_y + 2 * PADDING)\n        \n        surface_w = surface_w_raw + (surface_w_raw % 2) \n        surface_h = surface_h_raw + (surface_h_raw % 2) \n        \n        offset_x = -min_x + PADDING\n        offset_y = -min_y + PADDING\n\n        print(f\"  Raw Surface size for '{gloss}': {surface_w_raw}x{surface_h_raw}\")\n        print(f\"  Adjusted Even Surface size for '{gloss}': {surface_w}x{surface_h}\")\n        game_surface = pygame.Surface((surface_w, surface_h))\n\n        for frame_idx in range(num_frames):\n            game_surface.fill(BG_COLOR) \n            \n            current_frame_poses_orig = animation_poses[frame_idx]\n            current_frame_poses_transformed = np.copy(current_frame_poses_orig) # Use a different name\n            for kp_idx in range(num_keypoints):\n                current_frame_poses_transformed[kp_idx, 0] = current_frame_poses_orig[kp_idx, 0] + offset_x\n                current_frame_poses_transformed[kp_idx, 1] = current_frame_poses_orig[kp_idx, 1] + offset_y\n            \n            kps = {i: tuple(current_frame_poses_transformed[i]) for i in range(num_keypoints)}\n\n            if all(kp in kps for kp in [KP_L_SHOULDER, KP_R_SHOULDER, KP_L_HIP, KP_R_HIP]):\n                draw_torso_sprite(game_surface, kps[KP_L_SHOULDER], kps[KP_R_SHOULDER], \n                                  kps[KP_L_HIP], kps[KP_R_HIP], TORSO_COLOR)\n\n            limb_definitions = [\n                (KP_L_SHOULDER, KP_L_ELBOW), (KP_L_ELBOW, KP_L_WRIST), # Left Arm\n                (KP_R_SHOULDER, KP_R_ELBOW), (KP_R_ELBOW, KP_R_WRIST), # Right Arm\n                (KP_L_HIP, KP_L_KNEE), (KP_L_KNEE, KP_L_ANKLE),       # Left Leg\n                (KP_R_HIP, KP_R_KNEE), (KP_R_KNEE, KP_R_ANKLE)        # Right Leg\n            ]\n            for kp_start_idx, kp_end_idx in limb_definitions:\n                if kp_start_idx in kps and kp_end_idx in kps:\n                    draw_limb_sprite(game_surface, kps[kp_start_idx], kps[kp_end_idx], LIMB_COLOR, LIMB_WIDTH)\n            \n            if all(kp in kps for kp in [KP_NOSE, KP_L_SHOULDER, KP_R_SHOULDER]):\n                 draw_head_sprite(game_surface, kps[KP_NOSE], kps[KP_L_SHOULDER], kps[KP_R_SHOULDER], HEAD_COLOR, HEAD_RADIUS)\n            \n            try:\n                frame_pil_image = Image.frombytes('RGB', (surface_w, surface_h), pygame.image.tostring(game_surface, 'RGB'))\n                frame_pil_image.save(os.path.join(current_frames_dir, f\"frame_{frame_idx:05d}.png\"))\n            except Exception as e_save:\n                print(f\"    Error saving frame {frame_idx} for {gloss}: {e_save}\")\n                break \n        \n        output_video_path = os.path.join(PYGAME_ANIM_DIR, f\"pygame_anim_{gloss.replace('/', '_')}.mp4\")\n        ffmpeg_command = [\n            'ffmpeg', '-y', '-framerate', '20', \n            '-i', os.path.join(current_frames_dir, 'frame_%05d.png'),\n            '-c:v', 'libx264', '-pix_fmt', 'yuv420p', '-crf', '23', \n            output_video_path\n        ]\n        print(f\"  Attempting to compile video: {' '.join(ffmpeg_command)}\")\n        try:\n            result = subprocess.run(ffmpeg_command, check=False, capture_output=True, text=True) # check=False\n            if result.returncode == 0:\n                print(f\"  Successfully compiled video to {output_video_path}\")\n            else:\n                print(f\"  ffmpeg compilation FAILED for {gloss}. Return code: {result.returncode}\")\n                print(f\"    ffmpeg stdout:\\n{result.stdout}\")\n                print(f\"    ffmpeg stderr:\\n{result.stderr}\")\n\n        except FileNotFoundError:\n            print(\"  ERROR: ffmpeg command not found. Please ensure ffmpeg is installed and in your PATH.\")\n        except Exception as e_ffmpeg_general:\n            print(f\"  An unexpected error occurred during ffmpeg execution for {gloss}: {e_ffmpeg_general}\")\n            \n        processed_animations_count += 1\n\n    print(f\"\\nFinished generating {processed_animations_count} Pygame sprite animations in {PYGAME_ANIM_DIR}\")\n\nif not gloss_to_pose_map:\n    print(\"No glosses found to process.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:17:46.884046Z","iopub.execute_input":"2025-06-06T16:17:46.884389Z","iopub.status.idle":"2025-06-06T16:17:47.495469Z","shell.execute_reply.started":"2025-06-06T16:17:46.884368Z","shell.execute_reply":"2025-06-06T16:17:47.494793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: 2D Sprite Character Animation with Pygame (Skeletal Hands/Face)\nimport pygame\nimport numpy as np\nimport math\nimport os\nimport json\nfrom PIL import Image\nimport subprocess\n\n# --- Configuration ---\nPOSE_DATA_DIR = \"/kaggle/working/pose_data/\"\nPYGAME_ANIM_DIR = \"/kaggle/working/pygame_animations/\"\nPYGAME_FRAMES_DIR_BASE = \"/kaggle/working/pygame_frames_temp/\" # Base for temp frames\nGLOSS_TO_POSE_MAP_FILE = \"/kaggle/working/gloss_to_pose_map.json\"\nKEYPOINT_CONFIDENCE_THRESHOLD = 0.3\nNUM_PYGAME_ANIMATIONS_TO_GENERATE = 1\n\n# Colors\nBG_COLOR = (255, 255, 255)\nLIMB_COLOR = (50, 150, 255, 200)\nHEAD_COLOR_SHAPE = (255, 200, 150, 180) # Semi-transparent head circle\nTORSO_COLOR = (100, 100, 100, 200)\nHAND_SKEL_COLOR = (0, 200, 0) # Green for hand skeleton\nFACE_SKEL_COLOR = (200, 0, 200) # Purple for face skeleton\nJOINT_DEBUG_COLOR = (255,0,0)\n\n# Sprite/Shape Dimensions\nLIMB_WIDTH = 20\nHEAD_RADIUS = 25\nSKELETON_LINE_WIDTH = 3 # For hands and face\nKEYPOINT_DOT_RADIUS = 2 # For drawing hand/face keypoints\n\nPADDING = 50\n\n# --- Pygame Initialization ---\nif not pygame.display.get_init():\n    os.environ['SDL_VIDEODRIVER'] = 'dummy'\n    pygame.init()\n    print(\"Pygame initialized.\")\nelse:\n    print(\"Pygame already initialized.\")\n\n# --- Keypoint Indices (COCO WholeBody - 133 keypoints) ---\nKP_NOSE = 0; KP_L_EYE = 1; KP_R_EYE = 2; KP_L_EAR = 3; KP_R_EAR = 4\nKP_L_SHOULDER = 5; KP_R_SHOULDER = 6; KP_L_ELBOW = 7; KP_R_ELBOW = 8\nKP_L_WRIST = 9; KP_R_WRIST = 10; KP_L_HIP = 11; KP_R_HIP = 12\nKP_L_KNEE = 13; KP_R_KNEE = 14; KP_L_ANKLE = 15; KP_R_ANKLE = 16\n\n# Facial landmarks (subset from COCO-WholeBody points 23-90 for a more detailed face if needed)\n# For now, we'll stick to the basic ones + a simple mouth if points are available\n# Example mouth points (these are specific to COCO-WholeBody's 133 points)\nKP_MOUTH_L_CORNER = 78 # (Original index in 133 point model)\nKP_MOUTH_R_CORNER = 84\nKP_MOUTH_UPPER_LIP_TOP = 81\nKP_MOUTH_LOWER_LIP_BOTTOM = 87\n\n# Hand keypoint base indices\nKP_L_HAND_BASE = 91 # Left hand points are 91-111\nKP_R_HAND_BASE = 112 # Right hand points are 112-132\n\n# Hand skeleton links (local indices, 0-20 for one hand)\n# 0:Wrist, 1:ThumbCMC, 2:ThumbMCP, 3:ThumbIP, 4:ThumbTIP\n# 5:IndexMCP, 6:IndexPIP, 7:IndexDIP, 8:IndexTIP\n# ... and so on for Middle, Ring, Pinky\nHAND_SKELETON_LINKS = [\n    (0, 1), (1, 2), (2, 3), (3, 4),  # Thumb\n    (0, 5), (5, 6), (6, 7), (7, 8),  # Index\n    (0, 9), (9, 10), (10, 11), (11, 12), # Middle\n    (0, 13), (13, 14), (14, 15), (15, 16), # Ring\n    (0, 17), (17, 18), (18, 19), (19, 20)  # Pinky\n]\n# Connections from MCPs to each other for palm outline (optional)\n# HAND_PALM_LINKS = [(5,9), (9,13), (13,17)]\n\n\n# --- Helper Functions ---\ndef calculate_angle_degrees(p1x, p1y, p2x, p2y):\n    return math.degrees(math.atan2(p2y - p1y, p2x - p1x))\n\ndef calculate_distance(p1x, p1y, p2x, p2y):\n    return math.sqrt((p2x - p1x)**2 + (p2y - p1y)**2)\n\ndef draw_limb_sprite(surface, p1_data, p2_data, limb_color, limb_width):\n    x1, y1, conf1 = p1_data; x2, y2, conf2 = p2_data\n    if conf1 < KEYPOINT_CONFIDENCE_THRESHOLD or conf2 < KEYPOINT_CONFIDENCE_THRESHOLD: return\n    length = calculate_distance(x1, y1, x2, y2)\n    if length < 1: return\n    limb_surface = pygame.Surface((int(length), int(limb_width)), pygame.SRCALPHA)\n    limb_surface.fill(limb_color)\n    angle = calculate_angle_degrees(x1, y1, x2, y2)\n    rotated_limb_surface = pygame.transform.rotate(limb_surface, -angle)\n    rad_angle = math.radians(angle)\n    center_x = x1 + (length / 2.0) * math.cos(rad_angle)\n    center_y = y1 + (length / 2.0) * math.sin(rad_angle)\n    blit_rect = rotated_limb_surface.get_rect(center=(int(center_x), int(center_y)))\n    surface.blit(rotated_limb_surface, blit_rect.topleft)\n\ndef draw_head_circle(surface, nose_kp, l_shoulder_kp, r_shoulder_kp, head_color, radius):\n    nx, ny, nconf = nose_kp; lsx, lsy, lsconf = l_shoulder_kp; rsx, rsy, rsconf = r_shoulder_kp\n    if nconf < KEYPOINT_CONFIDENCE_THRESHOLD: return\n    head_center_x = nx; head_center_y = ny - radius * 0.5\n    if lsconf > KEYPOINT_CONFIDENCE_THRESHOLD and rsconf > KEYPOINT_CONFIDENCE_THRESHOLD:\n        shoulder_mid_y = (lsy + rsy) / 2\n        if ny > shoulder_mid_y: head_center_y = shoulder_mid_y - radius\n    pygame.draw.circle(surface, head_color, (int(head_center_x), int(head_center_y)), int(radius))\n\ndef draw_torso_sprite(surface, sl_kp, sr_kp, hl_kp, hr_kp, torso_color):\n    points_data = [sl_kp, sr_kp, hr_kp, hl_kp]; polygon_points = []\n    for p_data in points_data:\n        if p_data[2] > KEYPOINT_CONFIDENCE_THRESHOLD:\n            polygon_points.append((int(p_data[0]), int(p_data[1])))\n        else: return\n    if len(polygon_points) == 4: pygame.draw.polygon(surface, torso_color, polygon_points)\n\ndef draw_face_features(surface, kps, color, line_width, dot_radius):\n    \"\"\"Draws basic facial features: eyes, nose, simple mouth line.\"\"\"\n    # Nose dot\n    if KP_NOSE in kps and kps[KP_NOSE][2] > KEYPOINT_CONFIDENCE_THRESHOLD:\n        nx, ny, _ = kps[KP_NOSE]\n        pygame.draw.circle(surface, color, (int(nx), int(ny)), dot_radius)\n\n    # Eyes\n    for eye_kp_idx in [KP_L_EYE, KP_R_EYE]:\n        if eye_kp_idx in kps and kps[eye_kp_idx][2] > KEYPOINT_CONFIDENCE_THRESHOLD:\n            ex, ey, _ = kps[eye_kp_idx]\n            pygame.draw.circle(surface, color, (int(ex), int(ey)), dot_radius + 1) # Slightly larger eyes\n\n    # Simple mouth line (between corners, if available)\n    if KP_MOUTH_L_CORNER in kps and KP_MOUTH_R_CORNER in kps:\n        ml_x, ml_y, ml_conf = kps[KP_MOUTH_L_CORNER]\n        mr_x, mr_y, mr_conf = kps[KP_MOUTH_R_CORNER]\n        if ml_conf > KEYPOINT_CONFIDENCE_THRESHOLD and mr_conf > KEYPOINT_CONFIDENCE_THRESHOLD:\n            pygame.draw.line(surface, color, (int(ml_x), int(ml_y)), (int(mr_x), int(mr_y)), line_width)\n    # Or a line from nose to a chin point if mouth corners are not good. For now, just corners.\n\n\ndef draw_hand_skeleton(surface, kps, hand_base_idx, links, color, line_width, dot_radius):\n    \"\"\"Draws skeleton for one hand.\"\"\"\n    # Draw keypoints\n    for i in range(21): # 21 keypoints per hand\n        kp_idx = hand_base_idx + i\n        if kp_idx in kps and kps[kp_idx][2] > KEYPOINT_CONFIDENCE_THRESHOLD:\n            x, y, _ = kps[kp_idx]\n            pygame.draw.circle(surface, color, (int(x), int(y)), dot_radius)\n\n    # Draw links\n    for link_start_local, link_end_local in links:\n        kp_idx1 = hand_base_idx + link_start_local\n        kp_idx2 = hand_base_idx + link_end_local\n        if kp_idx1 in kps and kp_idx2 in kps:\n            x1, y1, conf1 = kps[kp_idx1]\n            x2, y2, conf2 = kps[kp_idx2]\n            if conf1 > KEYPOINT_CONFIDENCE_THRESHOLD and conf2 > KEYPOINT_CONFIDENCE_THRESHOLD:\n                pygame.draw.line(surface, color, (int(x1), int(y1)), (int(x2), int(y2)), line_width)\n\n# --- Load Data ---\ntry:\n    with open(GLOSS_TO_POSE_MAP_FILE, 'r') as f: gloss_to_pose_map = json.load(f)\n    print(f\"Successfully loaded gloss_to_pose_map from {GLOSS_TO_POSE_MAP_FILE}\")\nexcept Exception as e: print(f\"Error loading gloss_to_pose_map.json: {e}\"); gloss_to_pose_map = {}\n\nif not gloss_to_pose_map: print(\"Gloss map is empty. Cannot proceed.\")\nelse:\n    processed_animations_count = 0\n    for gloss, path_values in gloss_to_pose_map.items():\n        if processed_animations_count >= NUM_PYGAME_ANIMATIONS_TO_GENERATE: break\n        print(f\"\\nProcessing gloss: {gloss}\")\n        \n        current_frames_dir = os.path.join(PYGAME_FRAMES_DIR_BASE, gloss.replace('/', '_'))\n        if os.path.exists(current_frames_dir):\n            for f_name in os.listdir(current_frames_dir): os.remove(os.path.join(current_frames_dir, f_name))\n        else: os.makedirs(current_frames_dir)\n\n        current_npz_path_str = path_values[0] if isinstance(path_values, list) and path_values else path_values if isinstance(path_values, str) else None\n        if not current_npz_path_str: print(f\"  Invalid path for '{gloss}'. Skip.\"); continue\n        \n        npz_file_path = os.path.join(\"/kaggle/working/\", current_npz_path_str) if \"pose_data/\" in current_npz_path_str else os.path.join(POSE_DATA_DIR, current_npz_path_str)\n        if not os.path.exists(npz_file_path): print(f\"  NPZ not found: {npz_file_path}. Skip.\"); continue\n        \n        try: animation_poses = np.load(npz_file_path)['poses']\n        except Exception as e: print(f\"  Error loading NPZ {npz_file_path}: {e}. Skip.\"); continue\n\n        num_frames, num_keypoints, _ = animation_poses.shape\n        if num_frames == 0: print(f\"  No frames in {npz_file_path}. Skip.\"); continue\n        \n        confident_coords_x = [animation_poses[f,k,0] for f in range(num_frames) for k in range(num_keypoints) if animation_poses[f,k,2] > KEYPOINT_CONFIDENCE_THRESHOLD]\n        confident_coords_y = [animation_poses[f,k,1] for f in range(num_frames) for k in range(num_keypoints) if animation_poses[f,k,2] > KEYPOINT_CONFIDENCE_THRESHOLD]\n        if not confident_coords_x: print(f\"  No confident kps for '{gloss}'. Skip.\"); continue\n\n        min_x,max_x=min(confident_coords_x),max(confident_coords_x); min_y,max_y=min(confident_coords_y),max(confident_coords_y)\n        surface_w_raw=int(max_x-min_x+2*PADDING); surface_h_raw=int(max_y-min_y+2*PADDING)\n        surface_w=surface_w_raw+(surface_w_raw%2); surface_h=surface_h_raw+(surface_h_raw%2)\n        offset_x=-min_x+PADDING; offset_y=-min_y+PADDING\n        print(f\"  Surf size: {surface_w}x{surface_h} (Raw: {surface_w_raw}x{surface_h_raw})\")\n        game_surface = pygame.Surface((surface_w, surface_h))\n\n        for frame_idx in range(num_frames):\n            game_surface.fill(BG_COLOR)\n            current_poses_orig = animation_poses[frame_idx]\n            kps = {i: (current_poses_orig[i,0]+offset_x, current_poses_orig[i,1]+offset_y, current_poses_orig[i,2]) for i in range(num_keypoints)}\n\n            if all(idx in kps for idx in [KP_L_SHOULDER, KP_R_SHOULDER, KP_L_HIP, KP_R_HIP]):\n                draw_torso_sprite(game_surface, kps[KP_L_SHOULDER], kps[KP_R_SHOULDER], kps[KP_L_HIP], kps[KP_R_HIP], TORSO_COLOR)\n            \n            body_limbs = [ (KP_L_SHOULDER, KP_L_ELBOW), (KP_L_ELBOW, KP_L_WRIST), (KP_R_SHOULDER, KP_R_ELBOW), (KP_R_ELBOW, KP_R_WRIST),\n                           (KP_L_HIP, KP_L_KNEE), (KP_L_KNEE, KP_L_ANKLE), (KP_R_HIP, KP_R_KNEE), (KP_R_KNEE, KP_R_ANKLE) ]\n            for idx1, idx2 in body_limbs:\n                if idx1 in kps and idx2 in kps: draw_limb_sprite(game_surface, kps[idx1], kps[idx2], LIMB_COLOR, LIMB_WIDTH)\n            \n            if all(idx in kps for idx in [KP_NOSE, KP_L_SHOULDER, KP_R_SHOULDER]): # Head circle drawn first\n                draw_head_circle(game_surface, kps[KP_NOSE], kps[KP_L_SHOULDER], kps[KP_R_SHOULDER], HEAD_COLOR_SHAPE, HEAD_RADIUS)\n\n            draw_face_features(game_surface, kps, FACE_SKEL_COLOR, SKELETON_LINE_WIDTH -1, KEYPOINT_DOT_RADIUS) # Face features on top of head circle\n            draw_hand_skeleton(game_surface, kps, KP_L_HAND_BASE, HAND_SKELETON_LINKS, HAND_SKEL_COLOR, SKELETON_LINE_WIDTH, KEYPOINT_DOT_RADIUS)\n            draw_hand_skeleton(game_surface, kps, KP_R_HAND_BASE, HAND_SKELETON_LINKS, HAND_SKEL_COLOR, SKELETON_LINE_WIDTH, KEYPOINT_DOT_RADIUS)\n\n            try:\n                Image.frombytes('RGB', (surface_w, surface_h), pygame.image.tostring(game_surface, 'RGB')).save(os.path.join(current_frames_dir, f\"frame_{frame_idx:05d}.png\"))\n            except Exception as e_save: print(f\"  Err save frame {frame_idx} for {gloss}: {e_save}\"); break\n        \n        output_video_path = os.path.join(PYGAME_ANIM_DIR, f\"pygame_anim_{gloss.replace('/', '_')}.mp4\")\n        ffmpeg_cmd = ['ffmpeg','-y','-framerate','20','-i',os.path.join(current_frames_dir,'frame_%05d.png'),'-c:v','libx264','-pix_fmt','yuv420p','-crf','23',output_video_path]\n        print(f\"  Compiling: {' '.join(ffmpeg_cmd)}\")\n        try:\n            result = subprocess.run(ffmpeg_cmd, check=False, capture_output=True, text=True)\n            if result.returncode == 0: print(f\"  Video OK: {output_video_path}\")\n            else: print(f\"  ffmpeg FAIL {gloss}. Code: {result.returncode}\\n  stderr: {result.stderr[:1000]}\")\n        except Exception as e_ff: print(f\"  ffmpeg EXCEPTION for {gloss}: {e_ff}\")\n            \n        processed_animations_count += 1\n    print(f\"\\nFinished. {processed_animations_count} Pygame animations in {PYGAME_ANIM_DIR}\")\n\nif not gloss_to_pose_map: print(\"No glosses found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T16:28:12.530064Z","iopub.execute_input":"2025-06-06T16:28:12.531064Z","iopub.status.idle":"2025-06-06T16:28:13.212829Z","shell.execute_reply.started":"2025-06-06T16:28:12.531036Z","shell.execute_reply":"2025-06-06T16:28:13.211916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}